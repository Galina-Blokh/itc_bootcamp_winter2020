{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CErGY84pXEyw"
   },
   "source": [
    "# Information Theory: Perplexed by Texts\n",
    "\n",
    "**In this exercise we will calculate some information-theoretic quantities using the text of classic books from the Gutenberg project.**\n",
    "\n",
    "**The following code uses NLTK (one of the standard NLP libraries) to load the text of all of the books in an archive of the Gutenberg project:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2238,
     "status": "ok",
     "timestamp": 1577138559562,
     "user": {
      "displayName": "Morris Alper",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCLV7sZCDtOJIG7iIpeoUuX-iFbelSNgRrZxouWaw=s64",
      "userId": "15842932163458061285"
     },
     "user_tz": -120
    },
    "id": "c0r4KD_uXd-D",
    "outputId": "0904f617-7aad-468a-9610-1612c5498324"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /home/gal/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "from nltk.corpus import gutenberg\n",
    "import re\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = re.sub(r'[^A-Za-z0-9\\.,!\\?\\'\" ]', '_', gutenberg.raw())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jT7VhOqllOyD"
   },
   "source": [
    "**We see that this text starts from \"Emma\" by Jane Austen. We applied some character normalization so that the set of characters used will be limited:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2225,
     "status": "ok",
     "timestamp": 1577138559563,
     "user": {
      "displayName": "Morris Alper",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCLV7sZCDtOJIG7iIpeoUuX-iFbelSNgRrZxouWaw=s64",
      "userId": "15842932163458061285"
     },
     "user_tz": -120
    },
    "id": "kN6qGgKVXWde",
    "outputId": "1883fe18-cd42-47b5-8919-32c6dcf71c28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_Emma by Jane Austen 1816___VOLUME I__CHAPTER I___Emma Woodhouse, handsome, clever, and rich, with a comfortable home_and happy disposition, seemed to unite some of the best blessings_of existence_ and had lived nearly twenty_one years in the world_with very little to distress or vex her.__She was the youngest of the two daughters of a most affectionate,_indulgent father_ and had, in consequence of her sister's marriage,_been mistress of his house from a very early period.  Her mother_had died too long ago for her to have more than an indistinct_remembrance of her caresses_ and her place had been supplied_by an excellent woman as governess, who had fallen little short_of a mother in affection.__Sixteen years had Miss Taylor been in Mr. Woodhouse's family,_less as a governess than a friend, very fond of both daughters,_but particularly of Emma.  Between _them_ it was more the intimacy_of sisters.  Even before Miss Taylor had ceased to hold the nominal_office of governess, the mildness o\n"
     ]
    }
   ],
   "source": [
    "print(text[0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4LQQ4mH0lZRw"
   },
   "source": [
    "**The following questions refer to the entire text contained in this variable (which contains the text of multiple books).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uxlK-ENFZwWh"
   },
   "source": [
    "## Part 1: Entropy and friends\n",
    "\n",
    "### Questions:\n",
    "\n",
    "**1. How many unique characters are used in the text?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unique chars from text are:\n",
      "\n",
      " ['v', 'a', '.', 'f', 'W', 'M', 'b', 'g', \"'\", 'B', 'J', 'R', '?', 'j', 'E', 'l', 'e', '8', 'N', 'x', 'q', '\"', 'C', 'n', 'r', 'T', 'i', 'k', '1', '0', 'G', '9', 'F', 'p', 'm', 'D', 't', 'c', '7', 'U', 'I', 'K', 'Q', 'Z', 'X', 'o', '4', 'y', '6', 'O', '2', ' ', 'P', 'V', 'L', 'S', ',', 'z', '5', 'w', '!', 'A', 'u', 's', 'd', '3', '_', 'H', 'Y', 'h']\n",
      "\n",
      "70 unique characters are used in the text\n"
     ]
    }
   ],
   "source": [
    "unique_chars = list(set(text))\n",
    "\n",
    "print(f'The unique chars from text are:\\n\\n {unique_chars}\\n')\n",
    "print(f'{len(unique_chars)} unique characters are used in the text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Calculate the information content of characters: \"a\", \"e\", \"x\", \" \".** \n",
    "\n",
    "$ùêº(ùë•_i)=‚àíùëôùëúùëî_2(ùëÉ(ùë•_i))$\n",
    "\n",
    "Let's calculate char_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = collections.defaultdict(int)\n",
    "for c in text:\n",
    "    d[c] += 1\n",
    "    \n",
    "ch = pd.DataFrame(d.items(),columns=['chars','count_ch'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we able to calculate probability of appearance character in the text and info content of needed characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chars</th>\n",
       "      <th>count_ch</th>\n",
       "      <th>proba</th>\n",
       "      <th>info_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>698889</td>\n",
       "      <td>0.059261</td>\n",
       "      <td>4.076763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>2000723</td>\n",
       "      <td>0.169649</td>\n",
       "      <td>2.559376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>e</td>\n",
       "      <td>1108892</td>\n",
       "      <td>0.094027</td>\n",
       "      <td>3.410779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>x</td>\n",
       "      <td>8981</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>10.358806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chars  count_ch     proba  info_content\n",
       "3      a    698889  0.059261      4.076763\n",
       "4          2000723  0.169649      2.559376\n",
       "9      e   1108892  0.094027      3.410779\n",
       "42     x      8981  0.000762     10.358806"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ch['proba'] = ch.count_ch/ch.count_ch.sum()\n",
    "\n",
    "ch['info_content'] = -np.log2(ch.proba)\n",
    "\n",
    "ch.loc[ch.chars.isin([\"a\", \"e\", \"x\", \" \"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do these values look reasonable?** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, these values look reasonable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why ?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because from the dataframe ch we see, that the higher count in the text, the higher probability of appearance, the lower the information content of character we have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Calculate the character-level entropy and perplexity of the text.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ùêª=‚àë(ùëÉ(ùë•_ùëñ)‚àóùêº(ùë•_ùëñ))$ <br>\n",
    "$PP = 2^H$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "character-level entropy of the text is  4.498874036765774\n",
      "\n",
      "perplexity of the text is 22.609764133453833\n"
     ]
    }
   ],
   "source": [
    "shannon_entropy = np.sum(ch.proba * ch.info_content)\n",
    "print(f'''character-level entropy of the text is  {shannon_entropy}\n",
    "\n",
    "perplexity of the text is {2**shannon_entropy}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. What would be the character-level entropy and perplexity if characters were uniformly distributed?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 70 chars in out text, so the probability of a character would be 1/70 and the entropy:\n",
    "<br>$ùêª=‚àí‚àë(ùëÉ(ùë•_i)‚àóùëôùëúùëî_2(ùëÉ(ùë•_i)))$\n",
    "Where $P = \\frac{1}{len(ch)} = \\frac{1}{70}  $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "uniformly distributed character-level entropy of the text is  6.129283016944966\n",
      "uniformly distributed perplexity of the text is 70.0\n"
     ]
    }
   ],
   "source": [
    "uniform_entropy = -len(ch)*(1/len(ch) * np.log2(1/len(ch)))\n",
    "\n",
    "print(f'''\n",
    "uniformly distributed character-level entropy of the text is  {uniform_entropy}\n",
    "uniformly distributed perplexity of the text is {2**uniform_entropy}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Is this higher or lower than the values in 3, and why?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is higher than values in Q3, this is the maximum value for the entropy with such distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. What is the cross-entropy of the uniform distribution over characters relative to the sample distribution? <br>(H(P,Q) where P is the uniform distribution and Q is the sample distribution)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$H(P,Q) = -\\sum_{i}P(x_i) * log_2 Q(x_i) $\n",
    "\n",
    "<br>\n",
    "$P = \\frac{1}{len(ch)} = \\frac{1}{70}$<br>\n",
    "\n",
    "\n",
    "$log_2 Q(x_i) =$  ch['inf_content']\n",
    "\n",
    "\n",
    "$H(P,Q) =  - \\frac{1}{70 } * \\sum(log_2(Q(x_i))) = \\frac{1}{70} * \\sum(log_2(1/Q(x_i))) $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the cross-entropy of the uniform distribution over characters relative \n",
      "to the sample distribution is  8.59586772830242\n"
     ]
    }
   ],
   "source": [
    "cross_entropy = (1/len(ch) * ch.info_content).sum()\n",
    "\n",
    "print(f''' the cross-entropy of the uniform distribution over characters relative \n",
    "to the sample distribution is  {cross_entropy}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**6. Now consider a distribution where every character has probability 1/6 of being a space (\" \") and 5/6 probability of being any other character (all with equal probability).** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ùêª(ùëÉ,ùëÑ)=ùëÉ((\" \")) ‚àó ùëôùëúùëî_2(ùëÑ(\" \")) + ‚àë(ùëÉ(ùë•_ùëñ)‚àóùëôùëúùëî_2(ùëÑ(ùë•_ùëñ)))$\n",
    "$= \\frac{1}{6} ‚àó ùëôùëúùëî_2(ùëÑ(\"\")) + ‚àë(\\frac{5}{6} ‚àó \\frac{1}{(len(ch) -1)} ‚àó ùëôùëúùëî_2(ùëÑ(ùë•_ùëñ)))$\n",
    "\n",
    "\n",
    "x - other different characters, not `space`-character"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the cross-entropy of this distribution relative to the sample distribution?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.66269031352059"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for all non-space-chars\n",
    "ch['cross_entropy_5_6'] = ch.info_content * 5/6 * 1/(len(ch)-1)\n",
    "\n",
    "#for `space`-chars\n",
    "ch.loc[ch.chars == ' ', 'cross_entropy_5_6'] = 1/6 * ch.info_content \n",
    "\n",
    "ch.cross_entropy_5_6.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Is this higher or lower than the value from question 5, and why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross-entropy this time is lower than in Q5\n",
    "<br>We were calculating it with different condition of probability "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dZiMLfQraoLo"
   },
   "source": [
    "**Not only are letters distributed unevenly in English, but they are also dependent on their neighbors. For example, the letter 'q' almost always appears before the letter 'u'. Let's examine some information-theoretic consequences of this:**\n",
    "\n",
    "### Bonus question:\n",
    "**7. (bonus) Calculate the mutual information of a character given its predecessor in the text.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mutual Information is defined as follows:\n",
    "\n",
    "$I(X,Y)= ‚àëP(x,y) * (log_2(\\frac{P(x,y)}{P(x)*P(y)})$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bZs56E1Lpb5e"
   },
   "source": [
    "**A *bigram* is a set of two adjacent characters - for example, the word \"queen\" contains bigrams \"qu\", \"ue\", \"ee\", and \"en\". We can get all the bigrams in our text as follows:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UdiBxESapsAa"
   },
   "outputs": [],
   "source": [
    "bigrams_list = [text[i:i+2] for i in range(len(text)) if i < len(text) - 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, Let's Calculate the mutual information of a character given its predecessor in the text  <br>For this question for more convenient use in calculations  we will create a df here, It will help to follow the logic of calculations very clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigram</th>\n",
       "      <th>count_bi</th>\n",
       "      <th>proba_bi</th>\n",
       "      <th>info_content_bi</th>\n",
       "      <th>c_1</th>\n",
       "      <th>c_2</th>\n",
       "      <th>proba_c_1</th>\n",
       "      <th>proba_c_2</th>\n",
       "      <th>mutual_info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>_E</td>\n",
       "      <td>1740</td>\n",
       "      <td>1.475412e-04</td>\n",
       "      <td>12.726595</td>\n",
       "      <td>_</td>\n",
       "      <td>E</td>\n",
       "      <td>0.035187</td>\n",
       "      <td>0.000909</td>\n",
       "      <td>3.253284e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Em</td>\n",
       "      <td>991</td>\n",
       "      <td>8.403064e-05</td>\n",
       "      <td>13.538725</td>\n",
       "      <td>E</td>\n",
       "      <td>m</td>\n",
       "      <td>0.000909</td>\n",
       "      <td>0.018184</td>\n",
       "      <td>1.970717e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mm</td>\n",
       "      <td>5058</td>\n",
       "      <td>4.288870e-04</td>\n",
       "      <td>11.187115</td>\n",
       "      <td>m</td>\n",
       "      <td>m</td>\n",
       "      <td>0.018184</td>\n",
       "      <td>0.018184</td>\n",
       "      <td>1.609416e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ma</td>\n",
       "      <td>31552</td>\n",
       "      <td>2.675414e-03</td>\n",
       "      <td>8.546022</td>\n",
       "      <td>m</td>\n",
       "      <td>a</td>\n",
       "      <td>0.018184</td>\n",
       "      <td>0.059261</td>\n",
       "      <td>3.509941e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a</td>\n",
       "      <td>33356</td>\n",
       "      <td>2.828382e-03</td>\n",
       "      <td>8.465808</td>\n",
       "      <td>a</td>\n",
       "      <td></td>\n",
       "      <td>0.059261</td>\n",
       "      <td>0.169649</td>\n",
       "      <td>-5.175001e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2063</th>\n",
       "      <td>,t</td>\n",
       "      <td>1</td>\n",
       "      <td>8.479379e-08</td>\n",
       "      <td>23.491466</td>\n",
       "      <td>,</td>\n",
       "      <td>t</td>\n",
       "      <td>0.016312</td>\n",
       "      <td>0.067873</td>\n",
       "      <td>-1.159343e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2064</th>\n",
       "      <td>,a</td>\n",
       "      <td>2</td>\n",
       "      <td>1.695876e-07</td>\n",
       "      <td>22.491466</td>\n",
       "      <td>,</td>\n",
       "      <td>a</td>\n",
       "      <td>0.016312</td>\n",
       "      <td>0.059261</td>\n",
       "      <td>-2.115901e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2065</th>\n",
       "      <td>KX</td>\n",
       "      <td>1</td>\n",
       "      <td>8.479379e-08</td>\n",
       "      <td>23.491466</td>\n",
       "      <td>K</td>\n",
       "      <td>X</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>4.261806e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2066</th>\n",
       "      <td>'6</td>\n",
       "      <td>1</td>\n",
       "      <td>8.479379e-08</td>\n",
       "      <td>23.491466</td>\n",
       "      <td>'</td>\n",
       "      <td>6</td>\n",
       "      <td>0.001838</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>-3.046169e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2067</th>\n",
       "      <td>'4</td>\n",
       "      <td>1</td>\n",
       "      <td>8.479379e-08</td>\n",
       "      <td>23.491466</td>\n",
       "      <td>'</td>\n",
       "      <td>4</td>\n",
       "      <td>0.001838</td>\n",
       "      <td>0.000787</td>\n",
       "      <td>-3.471238e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2068 rows √ó 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     bigram  count_bi      proba_bi  info_content_bi c_1 c_2  proba_c_1  \\\n",
       "0        _E      1740  1.475412e-04        12.726595   _   E   0.035187   \n",
       "1        Em       991  8.403064e-05        13.538725   E   m   0.000909   \n",
       "2        mm      5058  4.288870e-04        11.187115   m   m   0.018184   \n",
       "3        ma     31552  2.675414e-03         8.546022   m   a   0.018184   \n",
       "4        a      33356  2.828382e-03         8.465808   a       0.059261   \n",
       "...     ...       ...           ...              ...  ..  ..        ...   \n",
       "2063     ,t         1  8.479379e-08        23.491466   ,   t   0.016312   \n",
       "2064     ,a         2  1.695876e-07        22.491466   ,   a   0.016312   \n",
       "2065     KX         1  8.479379e-08        23.491466   K   X   0.000171   \n",
       "2066     '6         1  8.479379e-08        23.491466   '   6   0.001838   \n",
       "2067     '4         1  8.479379e-08        23.491466   '   4   0.001838   \n",
       "\n",
       "      proba_c_2   mutual_info  \n",
       "0      0.000909  3.253284e-04  \n",
       "1      0.018184  1.970717e-04  \n",
       "2      0.018184  1.609416e-04  \n",
       "3      0.059261  3.509941e-03  \n",
       "4      0.169649 -5.175001e-03  \n",
       "...         ...           ...  \n",
       "2063   0.067873 -1.159343e-06  \n",
       "2064   0.059261 -2.115901e-06  \n",
       "2065   0.000015  4.261806e-07  \n",
       "2066   0.000556 -3.046169e-07  \n",
       "2067   0.000787 -3.471238e-07  \n",
       "\n",
       "[2068 rows x 9 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count the bigrams\n",
    "d = collections.defaultdict(int)\n",
    "for c in bigrams_list:\n",
    "    d[c] += 1\n",
    "\n",
    "#create df\n",
    "bi = pd.DataFrame(d.items(),columns=['bigram','count_bi'])\n",
    "\n",
    "#probabilities into new df (P(x))\n",
    "bi['proba_bi'] = bi.count_bi / bi.count_bi.sum()\n",
    "\n",
    "#info_content into new df \n",
    "bi['info_content_bi'] = np.log2(1/bi.proba_bi)\n",
    "\n",
    "#chars from bigrams into df\n",
    "bi['c_1'] = bi.bigram.astype(str).str[0]\n",
    "bi['c_2'] = bi.bigram.astype(str).str[1]\n",
    "\n",
    "#add proba char 1\n",
    "bi =bi.merge(ch[[\"proba\",\"chars\"]], left_on=\"c_1\",right_on='chars', how=\"left\")\n",
    "bi.drop(\"chars\", axis =1, inplace = True)\n",
    "bi.rename(columns={\"proba\": \"proba_c_1\"}, inplace=True)\n",
    "\n",
    "#add proba char 2\n",
    "bi = bi.merge(ch[[\"proba\",\"chars\"]], left_on=\"c_2\",right_on='chars', how=\"left\")\n",
    "bi.drop(\"chars\", axis =1, inplace = True)\n",
    "bi.rename(columns={\"proba\": \"proba_c_2\"}, inplace=True)\n",
    "\n",
    "#counting mutual information by formula P(x,y) * (log(P(x,y)/P(x)*P(y))\n",
    "bi['mutual_info']= bi.proba_bi * np.log2(bi.proba_bi / (bi.proba_c_1 * bi.proba_c_2))\n",
    "\n",
    "bi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What would this be if characters were all independent?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If ${X }$ and ${ Y }$ are Independent then  $I(X:Y)= 0$ since ${P(x,y)}={P(x)*P(y)}$ There is no Mutual Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BGl-XGZppvF9"
   },
   "source": [
    "### Questions:\n",
    "**8. Calculate entropy and perplexity of the *bigrams* in the text.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We defined all formulas for entropy and perplexity  in first questions so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.036241352863215"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy_bigrams = np.sum(bi.proba_bi * bi.info_content_bi)\n",
    "entropy_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262.5123257156022"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity_bigrams = 2**entropy_bigrams\n",
    "perplexity_bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. What would the entropy and perplexity be if all unique bigrams found in the text were uniformly distributed?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From df `bi` we know that we have 2068 unique bigrams. We will do calculations the same way as we were doing in first questions about characters. $P=  \\frac{1}{len(bi)} = \\frac{1}{2068}  $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.014020470314934"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#H = -len(ch)*(1/len(ch) * np.log2(1/len(ch)))\n",
    "entropy_uniform_unique_bigrams = -len(bi)*(1/len(bi)*np.log2(1/len(bi)))\n",
    "entropy_uniform_unique_bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the  perplexity if all unique bigrams found in the text were uniformly distributed is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2067.999999999999"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity_uniform_unique_bigrams = 2**entropy_uniform_unique_bigrams\n",
    "perplexity_uniform_unique_bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same as in the first questions about characters, we got perplexity equals the number of unique bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HqQ3iVFkbA4r"
   },
   "source": [
    "## Part 2: Coding theory\n",
    "\n",
    "**ASCII (American Standard Code for Information Interchange) is a coding standard for representing basic characters as a sequence of seven binary digits. For example, in ASCII the character \"a\" is represented as 1100001.**\n",
    "\n",
    "**Since the number of characters needed in everyday use of computers has expanded greatly since ASCII was released in 1963, text is usually encoded with more bits (e.g. in UTF-8 encoding). ASCII cannot represent characters in languages with non-Latin character sets or various other special characters.**\n",
    "\n",
    "### Questions:\n",
    "\n",
    "**10. What is the ASCII binary representation of the following string: \"HelloWorld\"? Use the Python functions bin() ord() to find the binary representation - do not do this manually.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01001000011001010110110001101100011011110101011101101111011100100110110001100100\n"
     ]
    }
   ],
   "source": [
    "def string2bits(s=''):\n",
    "    return ''.join([bin(ord(x))[2:].zfill(8) for x in s])\n",
    "\n",
    "\n",
    "s =  \"HelloWorld\"\n",
    "b = string2bits(s)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**11. What is the number of bits required in ASCII total and per character to represent our text?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ASCII character set contains a total of 128 characters and each character has a unique value between 0 and 127. Hence a 7-bit binary number is sufficient to represent a character from ASCII charset since a 7-bit binary number can hold values from 0 to 127 (total of 128 unique values ‚Üí ¬≤‚Å∑).\n",
    "\n",
    "The bit-width or bit-length is the length of the binary number used by an encoding scheme to represent a character. Hence in ASCII, the bit-width is 7.\n",
    "\n",
    "However, in a typical computer system, the memory is made of unit cells and each individual cell contains 8 bits (byte). Hence, even though ASCII needs only 7 bits to encode a character, it is stored as 8 bit by keeping first bit 0 (MSB). Hence, the actual bit-width of ASCII is 8.\n",
    "\n",
    "\n",
    "Since the first bit of ASCII character is always 0, it is also called as dead bit as it has no significance or use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94346544"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(string2bits(text))#for 8 bites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82553226"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)*7 #for 7 bites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**12. Is it possible to come up with a better fixed-length binary code for single characters in our text?** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No. Because of Fixed-length code  takes ‚åàlog 128‚åâ or 7 bits to code the 128 symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How about a better variable-length binary code? (You don't need to construct such codes, just state whether they exist and why.)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, we can use Huffman's algorithm:\n",
    "The output from Huffman's algorithm can be viewed as a variable-length code table for encoding a source symbol (such as a character in a file). The algorithm derives this table from the estimated probability or frequency of occurrence (weight) for each possible value of the source symbol. As in other entropy encoding methods, more common symbols are generally represented using fewer bits than less common symbols. Huffman's method can be efficiently implemented, finding a code in time linear to the number of input weights if these weights are sorted. However, although optimal among methods encoding symbols separately, Huffman coding is not always optimal among all compression methods - it is replaced with arithmetic coding or asymmetric numeral systems if better compression ratio is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**13. (bonus) What does the source-coding theorem predict should be approximately possible for our (long) text?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$N$ i.i.d. random variables each with entropy $H(X)$ can be compressed into more than $N‚ÄâH(X)$ bits with negligible risk of information loss, as $N ‚Üí ‚àû$; but conversely, if they are compressed into fewer than $N‚ÄâH(X)$ bits it is virtually certain that information will be lost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**14. (bonus) For the letters {\"a\", \"b\", \"c\", \"d\"}, why is the variable-length binary code {\"a\": \"011\", \"b\": \"0110\", \"c\": \"1110\", \"d\": \"10011\"} not uniquely decodeable? Give an example. <BR>Hint: calculate encodings of all possible words up to length 3 .**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the variable length code  {\"a\": \"011\", \"b\": \"0110\", \"c\": \"1110\", \"d\": \"10011\"} . A segment of encoded message such as ‚Äò01101110011‚Äô can be decoded in more than one way. For example, ‚Äò01101110011‚Äô can be interpreted in at least two ways, as $'aad'$ or as $'bca'$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code is not uniquely decodable and therefore cannot be used for data compression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Information Theory exercise.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
