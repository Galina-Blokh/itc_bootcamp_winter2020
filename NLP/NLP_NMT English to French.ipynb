{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "NMT_exercise_2_started.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-l0LiDewM6yV"
      },
      "source": [
        "# NMT Workshop Exercise 2: English to French\n",
        "\n",
        "<b>In this exercise we will train a seq2seq model to translate English sentences into French.\n",
        "\n",
        "First make sure you have the accompanying data file fra-eng.zip, and unzip it in the current directory.\n",
        "We will also need to download the spaCy model file for French.</b>\n",
        "\n",
        "##### Note for Google Colab users:\n",
        "<b>If you are working in Google Colab, uncomment the last line below before running, and restart the runtime after running the cell below. Also make sure you are using GPU acceleration for this exercise.</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-24T08:04:44.106013Z",
          "start_time": "2020-06-24T08:04:44.099925Z"
        },
        "colab_type": "code",
        "id": "cfcabdUoM6yY",
        "colab": {}
      },
      "source": [
        "# ! unzip fra-eng.zip\n",
        "# ! pip install tqdm==4.33.0"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jzEFex_rM6yd"
      },
      "source": [
        "<b>The file fra.txt contains English-French sentence pairs from the [Tatoeba project](https://tatoeba.org/eng/). Take a look at the contents of the file to see a few examples of sentence pairs in the corpus.\n",
        "\n",
        "For this exercise, we will also need spaCy models for English and French: (see [the spaCy documentation](https://spacy.io/usage/models) for more information)</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-24T08:04:44.765635Z",
          "start_time": "2020-06-24T08:04:44.763127Z"
        },
        "colab_type": "code",
        "id": "E_1P5UVcM6ye",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 979
        },
        "outputId": "88f6da9c-90e9-4988-d309-4a4bd44e8490"
      },
      "source": [
        "! python3 -m spacy download en\n",
        "! python3 -m spacy download fr"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (47.3.1)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.7.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.6.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "Requirement already satisfied: fr_core_news_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-2.2.5/fr_core_news_sm-2.2.5.tar.gz#egg=fr_core_news_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from fr_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (0.7.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (47.3.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.6.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('fr_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/fr_core_news_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/fr\n",
            "You can now load the model via spacy.load('fr')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-24T08:04:55.561030Z",
          "start_time": "2020-06-24T08:04:45.115859Z"
        },
        "colab_type": "code",
        "id": "yYlMrpnrM6yh",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "en_model = spacy.load('en')\n",
        "fr_model = spacy.load('fr')\n",
        "import pandas as pd"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HyDQ0KhHM6yk"
      },
      "source": [
        "## Part 1: Data preprocessing\n",
        "\n",
        "<b>First we will preprocess the data by taking a random sample, tokenizing, lowercasing, and removing sentences containing uncommon words. The goal is to have a clean sample set that will make learning faster.</b>\n",
        "\n",
        "### Questions:\n",
        "#### 1. Read the data in the file *fra.txt* into a Pandas dataframe *df* with columns *en* containing English sentences and *fr* containing the corresponding French sentences. <br>(Hint: use pandas.read_csv with parameters sep=, header=, usecols=, and names=). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-24T08:04:55.568612Z",
          "start_time": "2020-06-24T08:04:55.562624Z"
        },
        "colab_type": "code",
        "id": "SvAC1TV-FqWe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ef8a2a98-b085-4bf3-a99b-a83d7216229b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-24T08:04:56.457489Z",
          "start_time": "2020-06-24T08:04:55.572060Z"
        },
        "colab_type": "code",
        "id": "HLV2ZDEN_kF9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "8bd6fb02-0954-49d1-95b6-9fbbb67da0ac"
      },
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/fra.txt',\n",
        "                 usecols=[0,1] ,\n",
        "                 names=['en','fr'],\n",
        "                 sep='\\t', \n",
        "                 header=None)\n",
        "df.head(10)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>en</th>\n",
              "      <th>fr</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Go.</td>\n",
              "      <td>Va !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Hi.</td>\n",
              "      <td>Salut !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Hi.</td>\n",
              "      <td>Salut.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Run!</td>\n",
              "      <td>Cours !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Run!</td>\n",
              "      <td>Courez !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Who?</td>\n",
              "      <td>Qui ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Wow!</td>\n",
              "      <td>Ça alors !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Fire!</td>\n",
              "      <td>Au feu !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Help!</td>\n",
              "      <td>À l'aide !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Jump.</td>\n",
              "      <td>Saute.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      en          fr\n",
              "0    Go.        Va !\n",
              "1    Hi.     Salut !\n",
              "2    Hi.      Salut.\n",
              "3   Run!     Cours !\n",
              "4   Run!    Courez !\n",
              "5   Who?       Qui ?\n",
              "6   Wow!  Ça alors !\n",
              "7  Fire!    Au feu !\n",
              "8  Help!  À l'aide !\n",
              "9  Jump.      Saute."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KahPQUlhBLSR"
      },
      "source": [
        "#### How many sentence pairs are in the corpus?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-24T08:04:56.462194Z",
          "start_time": "2020-06-24T08:04:56.459209Z"
        },
        "colab_type": "code",
        "id": "WXffAJBq_kOt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7d7333e9-8815-4b47-bf0a-389530e630d5"
      },
      "source": [
        "len(df)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "174481"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Cv-AL6VzBLi8"
      },
      "source": [
        "#### 2. Filter to keep only sentences containing at most 40 characters (in both languages), and save a random sample of 50000 sentence pairs in a new dataframe *sample_df*.<br> Use pd.DataFrame.sample() with parameter random_state=0 to get reproduceable results.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-24T08:04:56.564831Z",
          "start_time": "2020-06-24T08:04:56.463365Z"
        },
        "colab_type": "code",
        "id": "D3ClSDL-_kUa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "outputId": "03707b88-4a5e-4b36-dd4e-cbee19e70248"
      },
      "source": [
        "sample_df = pd.DataFrame.sample(df[(df['en'].apply(len) <= 40) & \n",
        "                                (df['fr'].apply(len) <= 40)],\n",
        "                             n=50000,replace=False, random_state=0).reset_index(drop=True) \n",
        "sample_df"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>en</th>\n",
              "      <th>fr</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What did I do wrong?</td>\n",
              "      <td>Qu'ai-je fait de travers ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>You like olives, don't you?</td>\n",
              "      <td>Vous aimez les olives, pas vrai ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I really appreciate your coming.</td>\n",
              "      <td>J'apprécie vraiment ta venue.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I had to do something.</td>\n",
              "      <td>Il fallait que je fasse quelque chose.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I know Tom was first.</td>\n",
              "      <td>Je sais que Tom était le premier.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49995</th>\n",
              "      <td>He borrowed the car from his friend.</td>\n",
              "      <td>Il a emprunté la voiture à un ami.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49996</th>\n",
              "      <td>Send us a message.</td>\n",
              "      <td>Envoie-nous un message.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49997</th>\n",
              "      <td>Who's coming for dinner?</td>\n",
              "      <td>Qui vient dîner ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49998</th>\n",
              "      <td>You're lying!</td>\n",
              "      <td>Vous mentez !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49999</th>\n",
              "      <td>I always thought Tom was funny.</td>\n",
              "      <td>J'ai toujours pensé que Tom était drôle.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>50000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         en                                        fr\n",
              "0                      What did I do wrong?                Qu'ai-je fait de travers ?\n",
              "1               You like olives, don't you?         Vous aimez les olives, pas vrai ?\n",
              "2          I really appreciate your coming.             J'apprécie vraiment ta venue.\n",
              "3                    I had to do something.    Il fallait que je fasse quelque chose.\n",
              "4                     I know Tom was first.         Je sais que Tom était le premier.\n",
              "...                                     ...                                       ...\n",
              "49995  He borrowed the car from his friend.        Il a emprunté la voiture à un ami.\n",
              "49996                    Send us a message.                   Envoie-nous un message.\n",
              "49997              Who's coming for dinner?                         Qui vient dîner ?\n",
              "49998                         You're lying!                             Vous mentez !\n",
              "49999       I always thought Tom was funny.  J'ai toujours pensé que Tom était drôle.\n",
              "\n",
              "[50000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hlcznagtBLuA"
      },
      "source": [
        "#### 3. Add columns 'en_tokens' and 'fr_tokens' to sample_df containing arrays of the tokens in the English and French sentences. Use the spaCy models en_model and fr_model to tokenize the sentences, and make all the tokens lowercase. Add special tokens '&lt;start&gt;' and '&lt;end&gt;' to the beginning and end of every sentence.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-24T08:15:40.318446Z",
          "start_time": "2020-06-24T08:04:56.567142Z"
        },
        "colab_type": "code",
        "id": "z-VbwwOq_kei",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "36e857bc-b668-4a60-a1b1-529cb13ea473"
      },
      "source": [
        "def tokenize(model, sent):\n",
        "    doc = model.tokenizer(sent)\n",
        "    token_list = [token.text.lower() for token in doc]\n",
        "    token_list.append('<end>')\n",
        "    token_list.insert(0, '<start>')\n",
        "    return token_list\n",
        "\n",
        "sample_df['en_tokens'] = sample_df['en'].apply(lambda x: tokenize(en_model, x))\n",
        "sample_df['fr_tokens'] = sample_df['fr'].apply(lambda x: tokenize(fr_model, x))\n",
        "sample_df"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>en</th>\n",
              "      <th>fr</th>\n",
              "      <th>en_tokens</th>\n",
              "      <th>fr_tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What did I do wrong?</td>\n",
              "      <td>Qu'ai-je fait de travers ?</td>\n",
              "      <td>[&lt;start&gt;, what, did, i, do, wrong, ?, &lt;end&gt;]</td>\n",
              "      <td>[&lt;start&gt;, qu', ai, -, je, fait, de, travers, ?, &lt;end&gt;]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>You like olives, don't you?</td>\n",
              "      <td>Vous aimez les olives, pas vrai ?</td>\n",
              "      <td>[&lt;start&gt;, you, like, olives, ,, do, n't, you, ?, &lt;end&gt;]</td>\n",
              "      <td>[&lt;start&gt;, vous, aimez, les, olives, ,, pas, vrai, ?, &lt;end&gt;]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I really appreciate your coming.</td>\n",
              "      <td>J'apprécie vraiment ta venue.</td>\n",
              "      <td>[&lt;start&gt;, i, really, appreciate, your, coming, ., &lt;end&gt;]</td>\n",
              "      <td>[&lt;start&gt;, j', apprécie, vraiment, ta, venue, ., &lt;end&gt;]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I had to do something.</td>\n",
              "      <td>Il fallait que je fasse quelque chose.</td>\n",
              "      <td>[&lt;start&gt;, i, had, to, do, something, ., &lt;end&gt;]</td>\n",
              "      <td>[&lt;start&gt;, il, fallait, que, je, fasse, quelque, chose, ., &lt;end&gt;]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I know Tom was first.</td>\n",
              "      <td>Je sais que Tom était le premier.</td>\n",
              "      <td>[&lt;start&gt;, i, know, tom, was, first, ., &lt;end&gt;]</td>\n",
              "      <td>[&lt;start&gt;, je, sais, que, tom, était, le, premier, ., &lt;end&gt;]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49995</th>\n",
              "      <td>He borrowed the car from his friend.</td>\n",
              "      <td>Il a emprunté la voiture à un ami.</td>\n",
              "      <td>[&lt;start&gt;, he, borrowed, the, car, from, his, friend, ., &lt;end&gt;]</td>\n",
              "      <td>[&lt;start&gt;, il, a, emprunté, la, voiture, à, un, ami, ., &lt;end&gt;]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49996</th>\n",
              "      <td>Send us a message.</td>\n",
              "      <td>Envoie-nous un message.</td>\n",
              "      <td>[&lt;start&gt;, send, us, a, message, ., &lt;end&gt;]</td>\n",
              "      <td>[&lt;start&gt;, envoie, -, nous, un, message, ., &lt;end&gt;]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49997</th>\n",
              "      <td>Who's coming for dinner?</td>\n",
              "      <td>Qui vient dîner ?</td>\n",
              "      <td>[&lt;start&gt;, who, 's, coming, for, dinner, ?, &lt;end&gt;]</td>\n",
              "      <td>[&lt;start&gt;, qui, vient, dîner, ?, &lt;end&gt;]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49998</th>\n",
              "      <td>You're lying!</td>\n",
              "      <td>Vous mentez !</td>\n",
              "      <td>[&lt;start&gt;, you, 're, lying, !, &lt;end&gt;]</td>\n",
              "      <td>[&lt;start&gt;, vous, mentez, !, &lt;end&gt;]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49999</th>\n",
              "      <td>I always thought Tom was funny.</td>\n",
              "      <td>J'ai toujours pensé que Tom était drôle.</td>\n",
              "      <td>[&lt;start&gt;, i, always, thought, tom, was, funny, ., &lt;end&gt;]</td>\n",
              "      <td>[&lt;start&gt;, j', ai, toujours, pensé, que, tom, était, drôle, ., &lt;end&gt;]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>50000 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         en  ...                                                             fr_tokens\n",
              "0                      What did I do wrong?  ...                [<start>, qu', ai, -, je, fait, de, travers, ?, <end>]\n",
              "1               You like olives, don't you?  ...           [<start>, vous, aimez, les, olives, ,, pas, vrai, ?, <end>]\n",
              "2          I really appreciate your coming.  ...                [<start>, j', apprécie, vraiment, ta, venue, ., <end>]\n",
              "3                    I had to do something.  ...      [<start>, il, fallait, que, je, fasse, quelque, chose, ., <end>]\n",
              "4                     I know Tom was first.  ...           [<start>, je, sais, que, tom, était, le, premier, ., <end>]\n",
              "...                                     ...  ...                                                                   ...\n",
              "49995  He borrowed the car from his friend.  ...         [<start>, il, a, emprunté, la, voiture, à, un, ami, ., <end>]\n",
              "49996                    Send us a message.  ...                     [<start>, envoie, -, nous, un, message, ., <end>]\n",
              "49997              Who's coming for dinner?  ...                                [<start>, qui, vient, dîner, ?, <end>]\n",
              "49998                         You're lying!  ...                                     [<start>, vous, mentez, !, <end>]\n",
              "49999       I always thought Tom was funny.  ...  [<start>, j', ai, toujours, pensé, que, tom, était, drôle, ., <end>]\n",
              "\n",
              "[50000 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "A0ooqvSgBLoT"
      },
      "source": [
        "#### 4. Create sets en_common and fr_common containing the most common 2500 tokens in English and in French, respectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-24T11:36:12.483317Z",
          "start_time": "2020-06-24T11:36:12.355088Z"
        },
        "colab_type": "code",
        "id": "-juDMlP7_kjT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b40572e4-a398-439a-c094-15c8c32d4ec7"
      },
      "source": [
        "from collections import Counter\n",
        "from itertools import chain \n",
        "\n",
        "en_common = Counter(chain.from_iterable(sample_df['en_tokens'])).most_common(2500)\n",
        "\n",
        "fr_common = Counter(chain.from_iterable(sample_df['fr_tokens'])).most_common(2500)\n",
        "en_common = dict(en_common)\n",
        "en_common"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'<start>': 50000,\n",
              " '<end>': 50000,\n",
              " '.': 40884,\n",
              " 'i': 15748,\n",
              " 'you': 13495,\n",
              " '?': 8729,\n",
              " 'to': 8110,\n",
              " 'the': 6814,\n",
              " 'a': 6103,\n",
              " 'do': 5681,\n",
              " \"n't\": 5518,\n",
              " 'is': 5156,\n",
              " 'tom': 4430,\n",
              " 'it': 4163,\n",
              " \"'s\": 3880,\n",
              " 'that': 3870,\n",
              " 'he': 3428,\n",
              " 'this': 2866,\n",
              " 'me': 2803,\n",
              " 'have': 2729,\n",
              " 'are': 2503,\n",
              " 'we': 2460,\n",
              " 'was': 2399,\n",
              " 'of': 2387,\n",
              " 'what': 2275,\n",
              " \"'re\": 2236,\n",
              " \"'m\": 2221,\n",
              " 'my': 2149,\n",
              " 'did': 2054,\n",
              " 'your': 2031,\n",
              " 'in': 2025,\n",
              " 'be': 1737,\n",
              " 'she': 1638,\n",
              " 'not': 1635,\n",
              " 'for': 1634,\n",
              " 'want': 1611,\n",
              " 'like': 1564,\n",
              " 'know': 1427,\n",
              " 'they': 1417,\n",
              " ',': 1255,\n",
              " 'on': 1196,\n",
              " 'all': 1180,\n",
              " 'can': 1135,\n",
              " 'with': 1125,\n",
              " 'go': 1116,\n",
              " \"'ll\": 1114,\n",
              " 'how': 1062,\n",
              " 'very': 1016,\n",
              " 'here': 978,\n",
              " 'his': 971,\n",
              " \"'ve\": 959,\n",
              " 'at': 953,\n",
              " 'there': 928,\n",
              " 'no': 844,\n",
              " 'him': 839,\n",
              " 'will': 791,\n",
              " 'were': 787,\n",
              " 'think': 782,\n",
              " 'one': 773,\n",
              " 'about': 748,\n",
              " 'has': 743,\n",
              " 'going': 722,\n",
              " 'get': 719,\n",
              " 'up': 693,\n",
              " 'need': 677,\n",
              " 'who': 676,\n",
              " 'her': 670,\n",
              " 'ca': 667,\n",
              " 'good': 666,\n",
              " 'where': 665,\n",
              " 'why': 646,\n",
              " 'let': 644,\n",
              " 'out': 637,\n",
              " 'time': 598,\n",
              " 'does': 578,\n",
              " 'see': 566,\n",
              " 'come': 554,\n",
              " 'just': 552,\n",
              " 'really': 541,\n",
              " 'should': 541,\n",
              " 'had': 525,\n",
              " \"'d\": 524,\n",
              " 'would': 517,\n",
              " 'help': 517,\n",
              " '!': 501,\n",
              " 'got': 497,\n",
              " 'an': 493,\n",
              " 'mary': 478,\n",
              " 'tell': 458,\n",
              " 'us': 455,\n",
              " 'and': 454,\n",
              " 'take': 448,\n",
              " 'as': 447,\n",
              " 'so': 446,\n",
              " 'right': 443,\n",
              " 'could': 442,\n",
              " 'been': 424,\n",
              " 'too': 415,\n",
              " 'please': 411,\n",
              " 'never': 410,\n",
              " 'now': 410,\n",
              " 'love': 401,\n",
              " 'by': 393,\n",
              " 'from': 380,\n",
              " 'look': 378,\n",
              " 'make': 372,\n",
              " 'must': 369,\n",
              " 'give': 360,\n",
              " 'back': 347,\n",
              " 'work': 347,\n",
              " 'much': 339,\n",
              " 'some': 336,\n",
              " 'am': 334,\n",
              " 'talk': 331,\n",
              " 'more': 326,\n",
              " 'say': 315,\n",
              " 'something': 313,\n",
              " 'made': 313,\n",
              " 'car': 312,\n",
              " 'happy': 305,\n",
              " 'if': 305,\n",
              " 'still': 299,\n",
              " 'feel': 296,\n",
              " 'than': 294,\n",
              " 'home': 293,\n",
              " 'book': 286,\n",
              " 'leave': 284,\n",
              " 'anything': 282,\n",
              " 'lot': 280,\n",
              " 'thought': 277,\n",
              " 'money': 273,\n",
              " 'any': 272,\n",
              " 'doing': 269,\n",
              " 'again': 266,\n",
              " 'sure': 263,\n",
              " 'told': 262,\n",
              " 'alone': 261,\n",
              " 'when': 258,\n",
              " 'went': 255,\n",
              " 'off': 255,\n",
              " 'wo': 251,\n",
              " 'day': 246,\n",
              " 'everything': 245,\n",
              " 'said': 243,\n",
              " 'stop': 243,\n",
              " 'better': 243,\n",
              " 'eat': 238,\n",
              " 'them': 238,\n",
              " 'dog': 230,\n",
              " 'tomorrow': 229,\n",
              " 'today': 225,\n",
              " 'well': 224,\n",
              " 'room': 219,\n",
              " 'always': 217,\n",
              " 'long': 217,\n",
              " 'our': 216,\n",
              " 'house': 216,\n",
              " 'stay': 215,\n",
              " 'way': 215,\n",
              " 'may': 214,\n",
              " 'down': 214,\n",
              " 'left': 213,\n",
              " 'night': 212,\n",
              " 'these': 211,\n",
              " 'call': 211,\n",
              " 'yesterday': 210,\n",
              " 'little': 210,\n",
              " 'nothing': 210,\n",
              " 'busy': 204,\n",
              " 'speak': 204,\n",
              " 'job': 202,\n",
              " 'drink': 198,\n",
              " 'old': 197,\n",
              " 'friends': 197,\n",
              " 'done': 196,\n",
              " 'keep': 196,\n",
              " 'try': 195,\n",
              " 'put': 195,\n",
              " 'hope': 193,\n",
              " 'wrong': 190,\n",
              " 'wanted': 190,\n",
              " 'french': 190,\n",
              " 'believe': 189,\n",
              " 'over': 189,\n",
              " 'saw': 189,\n",
              " 'last': 188,\n",
              " 'friend': 187,\n",
              " 'door': 186,\n",
              " 'ready': 183,\n",
              " 'live': 181,\n",
              " 'name': 181,\n",
              " 'father': 181,\n",
              " 'new': 180,\n",
              " 'away': 178,\n",
              " 'find': 177,\n",
              " 'afraid': 176,\n",
              " 'everyone': 176,\n",
              " 'school': 176,\n",
              " 'enough': 175,\n",
              " 'problem': 175,\n",
              " 'man': 172,\n",
              " 'only': 169,\n",
              " 'read': 169,\n",
              " 'knew': 169,\n",
              " 'gave': 166,\n",
              " 'hear': 166,\n",
              " 'already': 166,\n",
              " 'hard': 165,\n",
              " 'three': 163,\n",
              " 'yet': 163,\n",
              " 'mother': 163,\n",
              " 'being': 161,\n",
              " 'anymore': 160,\n",
              " 'understand': 158,\n",
              " 'tired': 156,\n",
              " 'wait': 156,\n",
              " 'play': 155,\n",
              " 'buy': 155,\n",
              " 'many': 154,\n",
              " 'water': 153,\n",
              " 'boston': 153,\n",
              " 'two': 152,\n",
              " 'ask': 151,\n",
              " 'open': 150,\n",
              " 'fun': 149,\n",
              " 'took': 148,\n",
              " 'ever': 145,\n",
              " 'wants': 145,\n",
              " 'asked': 144,\n",
              " 'anyone': 143,\n",
              " 'seen': 143,\n",
              " 'nice': 143,\n",
              " 'people': 142,\n",
              " 'happened': 141,\n",
              " 'life': 141,\n",
              " 'bad': 141,\n",
              " 'married': 140,\n",
              " 'lost': 140,\n",
              " 'came': 139,\n",
              " 'big': 139,\n",
              " 'kind': 139,\n",
              " 'late': 138,\n",
              " 'yourself': 138,\n",
              " '-': 137,\n",
              " 'every': 137,\n",
              " 'everybody': 137,\n",
              " 'hate': 135,\n",
              " 'looking': 134,\n",
              " 'sorry': 133,\n",
              " 'care': 133,\n",
              " 'into': 132,\n",
              " 'idea': 131,\n",
              " 'bed': 130,\n",
              " 'watch': 129,\n",
              " 'used': 129,\n",
              " 'before': 129,\n",
              " 'often': 128,\n",
              " 'cold': 127,\n",
              " 'almost': 125,\n",
              " 'found': 123,\n",
              " 'best': 122,\n",
              " 'getting': 121,\n",
              " 'show': 121,\n",
              " 'great': 120,\n",
              " 'myself': 119,\n",
              " 'or': 118,\n",
              " 'sister': 116,\n",
              " 'talking': 116,\n",
              " 'children': 115,\n",
              " 'other': 115,\n",
              " 'plan': 114,\n",
              " 'true': 114,\n",
              " 'brother': 113,\n",
              " 'pay': 113,\n",
              " 'remember': 113,\n",
              " 'heard': 113,\n",
              " 'teacher': 113,\n",
              " 'first': 112,\n",
              " 'without': 112,\n",
              " 'hurt': 112,\n",
              " 'answer': 112,\n",
              " 'tried': 112,\n",
              " 'waiting': 111,\n",
              " 'mine': 110,\n",
              " 'knows': 109,\n",
              " 'sleep': 109,\n",
              " 'morning': 109,\n",
              " 'truth': 108,\n",
              " 'coming': 106,\n",
              " 'write': 106,\n",
              " 'beautiful': 106,\n",
              " 'seems': 105,\n",
              " 'someone': 105,\n",
              " 'yours': 104,\n",
              " 'bought': 104,\n",
              " 'early': 104,\n",
              " 'wish': 104,\n",
              " 'those': 103,\n",
              " 'mind': 103,\n",
              " 'even': 103,\n",
              " 'turn': 103,\n",
              " 'likes': 103,\n",
              " 'dinner': 102,\n",
              " 'tonight': 102,\n",
              " 'looks': 102,\n",
              " 'cat': 102,\n",
              " 'soon': 102,\n",
              " 'after': 101,\n",
              " 'felt': 101,\n",
              " 'another': 101,\n",
              " 'coffee': 101,\n",
              " 'else': 100,\n",
              " 'train': 99,\n",
              " 'use': 99,\n",
              " 'thing': 98,\n",
              " 'place': 98,\n",
              " 'same': 98,\n",
              " 'word': 98,\n",
              " 'bus': 97,\n",
              " 'next': 96,\n",
              " 'seem': 96,\n",
              " 'angry': 95,\n",
              " 'hand': 95,\n",
              " 'finished': 95,\n",
              " 'together': 94,\n",
              " 'books': 94,\n",
              " 'which': 94,\n",
              " 'happen': 94,\n",
              " 'years': 93,\n",
              " 'doctor': 93,\n",
              " 'both': 93,\n",
              " 'few': 92,\n",
              " 'nobody': 92,\n",
              " 'meet': 92,\n",
              " 'run': 91,\n",
              " 'forget': 91,\n",
              " 'boy': 91,\n",
              " 'parents': 91,\n",
              " 'question': 90,\n",
              " 'gone': 89,\n",
              " 'girl': 89,\n",
              " 'but': 89,\n",
              " 'party': 89,\n",
              " 'thank': 89,\n",
              " 'surprised': 88,\n",
              " 'own': 88,\n",
              " 'english': 88,\n",
              " 'around': 88,\n",
              " 'pretty': 88,\n",
              " 'letter': 87,\n",
              " 'easy': 86,\n",
              " 'died': 85,\n",
              " 'working': 84,\n",
              " 'things': 84,\n",
              " 'anybody': 84,\n",
              " 'careful': 82,\n",
              " 'change': 82,\n",
              " 'mistake': 81,\n",
              " 'their': 81,\n",
              " 'started': 80,\n",
              " 'meeting': 80,\n",
              " 'funny': 80,\n",
              " 'bit': 80,\n",
              " 'eyes': 80,\n",
              " 'safe': 80,\n",
              " 'wife': 80,\n",
              " 'each': 80,\n",
              " 'crazy': 79,\n",
              " 'reading': 79,\n",
              " 'hurry': 79,\n",
              " 'enjoy': 79,\n",
              " 'lunch': 78,\n",
              " 'looked': 78,\n",
              " 'lie': 77,\n",
              " 'family': 77,\n",
              " 'himself': 77,\n",
              " 'caught': 77,\n",
              " 'start': 76,\n",
              " 'proud': 76,\n",
              " 'quite': 76,\n",
              " 'fast': 76,\n",
              " 'music': 76,\n",
              " 'guy': 75,\n",
              " 'young': 75,\n",
              " 'story': 75,\n",
              " 'movie': 75,\n",
              " 'sick': 75,\n",
              " 'trying': 74,\n",
              " 'close': 74,\n",
              " 'born': 74,\n",
              " 'study': 74,\n",
              " 'swim': 74,\n",
              " 'hungry': 74,\n",
              " 'once': 74,\n",
              " 'miss': 73,\n",
              " 'wine': 73,\n",
              " 'turned': 73,\n",
              " 'drive': 73,\n",
              " 'serious': 72,\n",
              " 'win': 72,\n",
              " 'under': 72,\n",
              " 'beer': 71,\n",
              " 'bicycle': 71,\n",
              " 'stupid': 71,\n",
              " 'secret': 71,\n",
              " 'lives': 71,\n",
              " 'glad': 70,\n",
              " 'arrived': 70,\n",
              " 'tea': 70,\n",
              " 'phone': 70,\n",
              " 'window': 70,\n",
              " 'rain': 70,\n",
              " 'light': 70,\n",
              " 'forgot': 70,\n",
              " 'stand': 70,\n",
              " 'trust': 70,\n",
              " 'learn': 69,\n",
              " 'food': 69,\n",
              " 'picture': 69,\n",
              " 'advice': 69,\n",
              " 'die': 68,\n",
              " 'ran': 68,\n",
              " 'year': 68,\n",
              " 'child': 68,\n",
              " 'matter': 68,\n",
              " 'broke': 67,\n",
              " 'game': 67,\n",
              " 'favorite': 67,\n",
              " 'far': 66,\n",
              " 'ok': 66,\n",
              " 'handle': 66,\n",
              " 'fire': 65,\n",
              " 'park': 65,\n",
              " 'mean': 65,\n",
              " 'choice': 65,\n",
              " 'dress': 64,\n",
              " 'met': 64,\n",
              " 'days': 64,\n",
              " 'scared': 64,\n",
              " 'walk': 64,\n",
              " 'person': 64,\n",
              " 'daughter': 64,\n",
              " 'bring': 63,\n",
              " 'tree': 63,\n",
              " 'different': 63,\n",
              " 'hat': 63,\n",
              " 'hair': 62,\n",
              " 'free': 62,\n",
              " 'difficult': 62,\n",
              " 'sit': 62,\n",
              " 'chance': 61,\n",
              " 'loves': 61,\n",
              " 'exactly': 61,\n",
              " 'quickly': 61,\n",
              " 'playing': 61,\n",
              " 'week': 61,\n",
              " 'baby': 61,\n",
              " 'lose': 61,\n",
              " 'interested': 60,\n",
              " 'later': 60,\n",
              " 'decision': 60,\n",
              " 'liked': 59,\n",
              " 'station': 59,\n",
              " 'weather': 59,\n",
              " 'table': 59,\n",
              " 'worry': 59,\n",
              " 'says': 59,\n",
              " 'while': 59,\n",
              " 'agree': 59,\n",
              " 'sing': 59,\n",
              " 'hot': 59,\n",
              " 'quiet': 58,\n",
              " 'office': 58,\n",
              " 'kids': 58,\n",
              " 'thanks': 58,\n",
              " 'ten': 58,\n",
              " 'needs': 58,\n",
              " 'makes': 58,\n",
              " 'quit': 58,\n",
              " 'longer': 58,\n",
              " 'called': 58,\n",
              " 'important': 58,\n",
              " 'decided': 58,\n",
              " 'small': 58,\n",
              " 'shoes': 57,\n",
              " 'full': 57,\n",
              " 'strong': 57,\n",
              " 'son': 57,\n",
              " 'feeling': 56,\n",
              " 'dead': 55,\n",
              " 'able': 55,\n",
              " 'cake': 55,\n",
              " 'listen': 55,\n",
              " 'rich': 55,\n",
              " 'perfect': 55,\n",
              " 'business': 55,\n",
              " 'lucky': 55,\n",
              " 'might': 54,\n",
              " 'cost': 54,\n",
              " 'actually': 54,\n",
              " 'hit': 54,\n",
              " 'keys': 54,\n",
              " 'taking': 54,\n",
              " 'wearing': 54,\n",
              " 'maybe': 54,\n",
              " 'fish': 54,\n",
              " 'having': 54,\n",
              " 'shut': 54,\n",
              " '\"': 54,\n",
              " 'supposed': 53,\n",
              " 'invited': 53,\n",
              " 'hands': 53,\n",
              " 'moment': 53,\n",
              " 'red': 53,\n",
              " 'kept': 53,\n",
              " 'news': 53,\n",
              " 'clean': 53,\n",
              " 'box': 53,\n",
              " 'such': 53,\n",
              " 'milk': 52,\n",
              " 'tv': 52,\n",
              " 'clothes': 52,\n",
              " 'making': 52,\n",
              " 'ate': 52,\n",
              " 'stopped': 52,\n",
              " 'comes': 52,\n",
              " 'questions': 52,\n",
              " 'ago': 52,\n",
              " 'fell': 52,\n",
              " 'older': 52,\n",
              " 'real': 52,\n",
              " 'trouble': 51,\n",
              " 'key': 51,\n",
              " 'age': 51,\n",
              " 'missed': 51,\n",
              " 'changed': 51,\n",
              " 'against': 51,\n",
              " 'interesting': 51,\n",
              " 'minute': 51,\n",
              " 'somebody': 51,\n",
              " 'possible': 51,\n",
              " 'probably': 51,\n",
              " 'sense': 51,\n",
              " 'christmas': 51,\n",
              " 'behind': 50,\n",
              " 'wonder': 50,\n",
              " 'cook': 50,\n",
              " 'drunk': 50,\n",
              " 'dogs': 50,\n",
              " 'touch': 50,\n",
              " 'guess': 50,\n",
              " 'appreciate': 49,\n",
              " 'outside': 49,\n",
              " 'lying': 49,\n",
              " 'seat': 49,\n",
              " 'closed': 49,\n",
              " 'living': 49,\n",
              " 'homework': 49,\n",
              " 'heart': 49,\n",
              " 'monday': 49,\n",
              " 'paid': 48,\n",
              " 'deal': 48,\n",
              " 'tall': 48,\n",
              " 'smoking': 48,\n",
              " 'part': 48,\n",
              " 'tennis': 48,\n",
              " 'woman': 48,\n",
              " 'works': 48,\n",
              " 'dance': 48,\n",
              " 'promise': 48,\n",
              " 'times': 48,\n",
              " 'cup': 47,\n",
              " 'head': 47,\n",
              " 'australia': 47,\n",
              " 'through': 47,\n",
              " 'sound': 47,\n",
              " 'prefer': 47,\n",
              " 'talked': 47,\n",
              " 'smoke': 47,\n",
              " 'minutes': 46,\n",
              " 'town': 46,\n",
              " 'offer': 46,\n",
              " 'student': 46,\n",
              " 'mad': 46,\n",
              " 'girls': 46,\n",
              " 'hold': 46,\n",
              " 'killed': 46,\n",
              " 'boyfriend': 46,\n",
              " 'afternoon': 46,\n",
              " 'thinking': 46,\n",
              " 'worried': 46,\n",
              " 'students': 45,\n",
              " 'police': 45,\n",
              " 'follow': 45,\n",
              " 'dangerous': 45,\n",
              " 'become': 45,\n",
              " 'upset': 45,\n",
              " 'summer': 45,\n",
              " 'team': 45,\n",
              " 'girlfriend': 45,\n",
              " 'check': 45,\n",
              " 'short': 45,\n",
              " 'dream': 45,\n",
              " 'asleep': 45,\n",
              " 'high': 45,\n",
              " 'second': 45,\n",
              " 'five': 44,\n",
              " 'lied': 44,\n",
              " 'leaving': 44,\n",
              " 'number': 44,\n",
              " 'world': 43,\n",
              " 'mistakes': 43,\n",
              " 'city': 43,\n",
              " 'country': 43,\n",
              " 'kiss': 43,\n",
              " 'nervous': 43,\n",
              " 'husband': 43,\n",
              " 'expect': 43,\n",
              " 'coat': 43,\n",
              " 'prepared': 42,\n",
              " 'watching': 42,\n",
              " 'brought': 42,\n",
              " 'swimming': 42,\n",
              " 'glass': 42,\n",
              " 'rather': 42,\n",
              " 'death': 42,\n",
              " 'dark': 42,\n",
              " 'eating': 42,\n",
              " 'danger': 42,\n",
              " 'kill': 42,\n",
              " 'expensive': 41,\n",
              " 'boss': 41,\n",
              " 'fine': 41,\n",
              " 'began': 41,\n",
              " 'guys': 41,\n",
              " 'walked': 41,\n",
              " 'running': 41,\n",
              " 'song': 41,\n",
              " 'sad': 41,\n",
              " 'helped': 41,\n",
              " 'finish': 41,\n",
              " 'liar': 41,\n",
              " 'drank': 40,\n",
              " 'accident': 40,\n",
              " 'horse': 40,\n",
              " 'bag': 40,\n",
              " 'hotel': 40,\n",
              " 'sounds': 40,\n",
              " 'beach': 40,\n",
              " 'lived': 40,\n",
              " 'face': 40,\n",
              " 'catch': 40,\n",
              " 'broken': 40,\n",
              " 'either': 40,\n",
              " 'point': 40,\n",
              " 'allowed': 40,\n",
              " 'floor': 40,\n",
              " 'white': 40,\n",
              " 'warm': 40,\n",
              " 'break': 40,\n",
              " 'rest': 40,\n",
              " 'near': 39,\n",
              " 'move': 39,\n",
              " 'choose': 39,\n",
              " 'then': 39,\n",
              " 'shot': 39,\n",
              " 'cut': 39,\n",
              " 'present': 39,\n",
              " 'whole': 39,\n",
              " 'fix': 39,\n",
              " 'hour': 39,\n",
              " 'computer': 39,\n",
              " 'loved': 39,\n",
              " 'reason': 38,\n",
              " 'tie': 38,\n",
              " 'shirt': 38,\n",
              " 'apple': 38,\n",
              " 'address': 38,\n",
              " 'pen': 38,\n",
              " 'joke': 38,\n",
              " 'crying': 38,\n",
              " 'piano': 38,\n",
              " 'alive': 38,\n",
              " 'wake': 38,\n",
              " 'worked': 38,\n",
              " 'owe': 38,\n",
              " 'blame': 37,\n",
              " 'trip': 37,\n",
              " 'snow': 37,\n",
              " 'end': 37,\n",
              " 'luck': 37,\n",
              " 'weird': 37,\n",
              " 'speaking': 37,\n",
              " 'sleeping': 37,\n",
              " 'gun': 37,\n",
              " 'fault': 37,\n",
              " 'fat': 37,\n",
              " 'jealous': 36,\n",
              " 'words': 36,\n",
              " 'became': 36,\n",
              " 'sat': 36,\n",
              " 'noise': 36,\n",
              " 'waited': 36,\n",
              " 'writing': 36,\n",
              " 'smart': 36,\n",
              " 'learned': 36,\n",
              " 'drinking': 36,\n",
              " 'patient': 35,\n",
              " 'worth': 35,\n",
              " 'showed': 35,\n",
              " 'pain': 35,\n",
              " 'opinion': 35,\n",
              " 'sign': 35,\n",
              " 'eaten': 35,\n",
              " 'desk': 35,\n",
              " 'explain': 35,\n",
              " 'report': 35,\n",
              " 'wrote': 35,\n",
              " 'month': 35,\n",
              " 'inside': 35,\n",
              " 'situation': 35,\n",
              " 'blue': 35,\n",
              " 'line': 34,\n",
              " 'won': 34,\n",
              " 'empty': 34,\n",
              " 'air': 34,\n",
              " 'most': 34,\n",
              " 'necessary': 34,\n",
              " 'river': 34,\n",
              " 'black': 34,\n",
              " 'six': 34,\n",
              " 'telling': 34,\n",
              " 'list': 34,\n",
              " 'plane': 34,\n",
              " 'thirty': 34,\n",
              " 'join': 34,\n",
              " 'hours': 34,\n",
              " 'attention': 34,\n",
              " 'accept': 33,\n",
              " 'manager': 33,\n",
              " 'accused': 33,\n",
              " 'travel': 33,\n",
              " 'begin': 33,\n",
              " 'advised': 33,\n",
              " 'dressed': 33,\n",
              " 'seeing': 33,\n",
              " 'apples': 33,\n",
              " 'followed': 33,\n",
              " 'whose': 33,\n",
              " 'along': 33,\n",
              " 'hospital': 33,\n",
              " 'weight': 33,\n",
              " 'stayed': 33,\n",
              " 'set': 32,\n",
              " 'fair': 32,\n",
              " 'wall': 32,\n",
              " 'health': 32,\n",
              " 'strange': 32,\n",
              " 'lawyer': 32,\n",
              " 'laughed': 32,\n",
              " 'language': 32,\n",
              " 'studying': 32,\n",
              " 'terrible': 32,\n",
              " 'control': 32,\n",
              " 'cute': 32,\n",
              " 'somewhere': 32,\n",
              " 'dictionary': 32,\n",
              " 'waste': 32,\n",
              " 'goes': 32,\n",
              " 'road': 32,\n",
              " 'problems': 32,\n",
              " 'wonderful': 32,\n",
              " 'needed': 32,\n",
              " 'seemed': 31,\n",
              " 'ticket': 31,\n",
              " 'flowers': 31,\n",
              " 'laugh': 31,\n",
              " 'correct': 31,\n",
              " 'cool': 31,\n",
              " 'paper': 31,\n",
              " 'happening': 31,\n",
              " \"o'clock\": 31,\n",
              " 'uncle': 31,\n",
              " 'spoke': 31,\n",
              " 'slept': 31,\n",
              " 'lock': 31,\n",
              " 'breakfast': 31,\n",
              " 'known': 31,\n",
              " 'half': 31,\n",
              " 'cry': 31,\n",
              " 'none': 31,\n",
              " 'message': 31,\n",
              " 'clear': 30,\n",
              " 'saved': 30,\n",
              " 'happens': 30,\n",
              " 'figured': 30,\n",
              " 'rules': 30,\n",
              " 'college': 30,\n",
              " 'japanese': 30,\n",
              " 'evening': 30,\n",
              " 'poor': 30,\n",
              " 'leg': 30,\n",
              " 'act': 30,\n",
              " 'shower': 30,\n",
              " 'disappointed': 30,\n",
              " 'driving': 30,\n",
              " 'speaks': 30,\n",
              " 'pleased': 30,\n",
              " 'ahead': 30,\n",
              " 'japan': 30,\n",
              " 'borrow': 30,\n",
              " 'fight': 30,\n",
              " 'listening': 30,\n",
              " 'front': 30,\n",
              " 'deserve': 30,\n",
              " 'lake': 29,\n",
              " 'saying': 29,\n",
              " 'succeed': 29,\n",
              " 'famous': 29,\n",
              " 'visit': 29,\n",
              " 'excuse': 29,\n",
              " 'stolen': 29,\n",
              " 'fired': 29,\n",
              " 'movies': 29,\n",
              " 'ride': 29,\n",
              " 'refused': 29,\n",
              " 'store': 29,\n",
              " 'completely': 29,\n",
              " 'taken': 29,\n",
              " 'order': 29,\n",
              " 'until': 29,\n",
              " 'plenty': 29,\n",
              " 'boat': 28,\n",
              " 'bank': 28,\n",
              " 'sometimes': 28,\n",
              " 'painted': 28,\n",
              " 'umbrella': 28,\n",
              " 'also': 28,\n",
              " 'finally': 28,\n",
              " 'plans': 28,\n",
              " 'eggs': 28,\n",
              " 'building': 28,\n",
              " 'raining': 28,\n",
              " 'piece': 28,\n",
              " 'war': 28,\n",
              " 'immediately': 28,\n",
              " 'opened': 28,\n",
              " 'god': 28,\n",
              " 'played': 28,\n",
              " 'save': 27,\n",
              " 'normal': 27,\n",
              " 'wear': 27,\n",
              " 'convinced': 27,\n",
              " 'since': 27,\n",
              " 'hey': 27,\n",
              " 'bread': 27,\n",
              " 'camera': 27,\n",
              " 'class': 27,\n",
              " 'staying': 27,\n",
              " 'chair': 27,\n",
              " 'usually': 27,\n",
              " 'garden': 27,\n",
              " 'warned': 27,\n",
              " 'future': 27,\n",
              " 'weekend': 27,\n",
              " 'protect': 27,\n",
              " 'apologize': 27,\n",
              " 'street': 27,\n",
              " 'guilty': 27,\n",
              " 'mistaken': 27,\n",
              " 'taller': 27,\n",
              " 'favor': 27,\n",
              " 'singing': 27,\n",
              " 'bill': 27,\n",
              " 'smell': 27,\n",
              " 'date': 26,\n",
              " 'ideas': 26,\n",
              " 'glasses': 26,\n",
              " 'color': 26,\n",
              " 'test': 26,\n",
              " 'thinks': 26,\n",
              " 'promised': 26,\n",
              " 'duty': 26,\n",
              " 'totally': 26,\n",
              " 'success': 26,\n",
              " 'others': 26,\n",
              " 'laughing': 26,\n",
              " 'boring': 26,\n",
              " 'mom': 26,\n",
              " 'cats': 26,\n",
              " 'simple': 26,\n",
              " 'price': 26,\n",
              " 'fishing': 26,\n",
              " 'excited': 26,\n",
              " 'eye': 26,\n",
              " 'medicine': 25,\n",
              " 'restaurant': 25,\n",
              " 'pass': 25,\n",
              " 'ashamed': 25,\n",
              " 'cooking': 25,\n",
              " 'sleepy': 25,\n",
              " 'enjoyed': 25,\n",
              " 'john': 25,\n",
              " 'suddenly': 25,\n",
              " 'crime': 25,\n",
              " 'step': 25,\n",
              " 'plays': 25,\n",
              " 'ordered': 25,\n",
              " 'lazy': 25,\n",
              " 'drop': 25,\n",
              " 'deep': 25,\n",
              " 'whatever': 25,\n",
              " 'radio': 25,\n",
              " 'written': 25,\n",
              " 'vacation': 25,\n",
              " 'sugar': 25,\n",
              " 'charge': 25,\n",
              " 'passed': 25,\n",
              " 'honest': 25,\n",
              " 'surprise': 25,\n",
              " 'shopping': 24,\n",
              " 'satisfied': 24,\n",
              " 'paint': 24,\n",
              " 'acting': 24,\n",
              " 'lonely': 24,\n",
              " 'green': 24,\n",
              " 'golf': 24,\n",
              " 'pictures': 24,\n",
              " 'twice': 24,\n",
              " 'kissed': 24,\n",
              " 'tokyo': 24,\n",
              " 'asking': 24,\n",
              " 'brothers': 24,\n",
              " 'confused': 24,\n",
              " 'bottle': 24,\n",
              " 'naive': 24,\n",
              " 'awesome': 24,\n",
              " 'beginning': 24,\n",
              " 'map': 24,\n",
              " 'forced': 24,\n",
              " 'nose': 24,\n",
              " 'missing': 24,\n",
              " 'slow': 24,\n",
              " '2:30': 24,\n",
              " 'admit': 24,\n",
              " 'neighbors': 24,\n",
              " 'blood': 24,\n",
              " 'moved': 24,\n",
              " 'lend': 24,\n",
              " 'teeth': 24,\n",
              " 'stuff': 24,\n",
              " 'grow': 24,\n",
              " 'sent': 24,\n",
              " 'sun': 24,\n",
              " 'belong': 24,\n",
              " 'teach': 24,\n",
              " 'tastes': 24,\n",
              " 'hiding': 23,\n",
              " 'yes': 23,\n",
              " 'calm': 23,\n",
              " 'sweet': 23,\n",
              " 'given': 23,\n",
              " 'feels': 23,\n",
              " 'bored': 23,\n",
              " 'bet': 23,\n",
              " 'case': 23,\n",
              " 'forward': 23,\n",
              " 'stole': 23,\n",
              " 'couple': 23,\n",
              " 'perfectly': 23,\n",
              " 'decide': 23,\n",
              " 'sell': 23,\n",
              " 'impressed': 23,\n",
              " 'worse': 23,\n",
              " 'count': 23,\n",
              " 'prison': 23,\n",
              " 'fool': 23,\n",
              " 'kid': 23,\n",
              " 'suppose': 23,\n",
              " 'arm': 23,\n",
              " 'sold': 23,\n",
              " 'men': 23,\n",
              " 'escape': 23,\n",
              " 'solve': 23,\n",
              " 'certain': 23,\n",
              " 'comfortable': 22,\n",
              " 'its': 22,\n",
              " 'nuts': 22,\n",
              " 'skiing': 22,\n",
              " 'large': 22,\n",
              " 'sisters': 22,\n",
              " 'innocent': 22,\n",
              " 'bike': 22,\n",
              " 'imagination': 22,\n",
              " 'peace': 22,\n",
              " 'abroad': 22,\n",
              " 'meal': 22,\n",
              " 'carry': 22,\n",
              " 'bother': 22,\n",
              " 'october': 22,\n",
              " ...}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-24T11:36:13.402567Z",
          "start_time": "2020-06-24T11:36:13.373112Z"
        },
        "colab_type": "code",
        "id": "DUDzA1M7teH5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4e0fd617-6ad5-498f-bcaf-13f1e0982e51"
      },
      "source": [
        "fr_common = dict(fr_common)\n",
        "fr_common"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'<start>': 50000,\n",
              " '<end>': 50000,\n",
              " '.': 39354,\n",
              " 'je': 10932,\n",
              " '?': 8732,\n",
              " 'est': 7726,\n",
              " '-': 7644,\n",
              " 'vous': 6603,\n",
              " 'de': 6579,\n",
              " 'pas': 6551,\n",
              " 'tu': 5186,\n",
              " 'il': 5150,\n",
              " 'ne': 4835,\n",
              " 'le': 4632,\n",
              " 'tom': 4413,\n",
              " 'que': 4408,\n",
              " 'à': 4267,\n",
              " \"j'\": 4241,\n",
              " 'a': 4155,\n",
              " 'la': 3852,\n",
              " \"n'\": 3728,\n",
              " 'ai': 3634,\n",
              " 'un': 3484,\n",
              " \"l'\": 3165,\n",
              " 'nous': 2896,\n",
              " 'ce': 2585,\n",
              " 'en': 2560,\n",
              " \"c'\": 2329,\n",
              " 'une': 2251,\n",
              " '!': 2024,\n",
              " 'suis': 2002,\n",
              " \"d'\": 1998,\n",
              " 'me': 1900,\n",
              " ',': 1897,\n",
              " '\\u202f': 1874,\n",
              " 'ça': 1827,\n",
              " '\\xa0': 1718,\n",
              " 'elle': 1679,\n",
              " 'les': 1651,\n",
              " 'faire': 1321,\n",
              " \"m'\": 1307,\n",
              " 'moi': 1267,\n",
              " \"qu'\": 1245,\n",
              " 'y': 1228,\n",
              " 'êtes': 1216,\n",
              " 'tout': 1185,\n",
              " 'veux': 1173,\n",
              " 'as': 1160,\n",
              " 'pour': 1147,\n",
              " 'te': 1130,\n",
              " 'es': 1066,\n",
              " 'qui': 1041,\n",
              " 'fait': 1019,\n",
              " 'était': 1016,\n",
              " \"s'\": 1012,\n",
              " 'mon': 1000,\n",
              " 'être': 991,\n",
              " '-ce': 979,\n",
              " 'plus': 964,\n",
              " 'avez': 929,\n",
              " 'très': 899,\n",
              " 'ils': 878,\n",
              " 'dans': 858,\n",
              " 'des': 834,\n",
              " 'sont': 833,\n",
              " 'du': 812,\n",
              " 'avec': 804,\n",
              " 'au': 772,\n",
              " 'se': 772,\n",
              " 'cela': 759,\n",
              " \"t'\": 759,\n",
              " 'ici': 753,\n",
              " 'sais': 726,\n",
              " 'cette': 699,\n",
              " 'peux': 697,\n",
              " 'été': 684,\n",
              " 'où': 682,\n",
              " 'ma': 676,\n",
              " 'toi': 675,\n",
              " 'votre': 666,\n",
              " 'son': 649,\n",
              " 'pourquoi': 638,\n",
              " 'aime': 635,\n",
              " 'si': 628,\n",
              " 'comment': 614,\n",
              " 'dit': 591,\n",
              " 'ton': 573,\n",
              " 'vraiment': 572,\n",
              " 'lui': 562,\n",
              " 'elles': 559,\n",
              " 'on': 530,\n",
              " 'bien': 529,\n",
              " 'ont': 523,\n",
              " 'pense': 513,\n",
              " 'sommes': 493,\n",
              " 'là': 492,\n",
              " 'avons': 484,\n",
              " 'jamais': 483,\n",
              " 'sur': 480,\n",
              " 'personne': 471,\n",
              " 'besoin': 467,\n",
              " 'rien': 459,\n",
              " 'temps': 454,\n",
              " 'tous': 450,\n",
              " 'va': 443,\n",
              " 'aller': 436,\n",
              " 'et': 433,\n",
              " 'beaucoup': 416,\n",
              " 'dois': 408,\n",
              " 'bon': 400,\n",
              " 'chose': 391,\n",
              " 'sa': 387,\n",
              " 'étais': 383,\n",
              " 'avoir': 379,\n",
              " 'peut': 376,\n",
              " 'parler': 375,\n",
              " 'encore': 367,\n",
              " 'voir': 367,\n",
              " 'faut': 361,\n",
              " 'ta': 356,\n",
              " 'quelque': 354,\n",
              " 'trop': 354,\n",
              " 'mes': 353,\n",
              " 'air': 350,\n",
              " 'dire': 349,\n",
              " 'vais': 340,\n",
              " 'mary': 340,\n",
              " 'monde': 338,\n",
              " 'toujours': 326,\n",
              " 'peu': 323,\n",
              " 'fais': 321,\n",
              " 'ceci': 317,\n",
              " 'par': 310,\n",
              " 'maison': 309,\n",
              " 'quel': 308,\n",
              " 'comme': 306,\n",
              " 'quoi': 303,\n",
              " 'voiture': 299,\n",
              " 'même': 298,\n",
              " 'vu': 287,\n",
              " 'train': 285,\n",
              " 'aimerais': 283,\n",
              " 'maintenant': 281,\n",
              " 'livre': 277,\n",
              " 'deux': 274,\n",
              " 'argent': 265,\n",
              " 'déjà': 263,\n",
              " 'aider': 261,\n",
              " 'juste': 260,\n",
              " 'hier': 258,\n",
              " 'quelle': 253,\n",
              " 'ses': 249,\n",
              " 'quand': 246,\n",
              " 'porte': 246,\n",
              " 'puis': 245,\n",
              " 'travail': 240,\n",
              " 'autre': 237,\n",
              " 'combien': 236,\n",
              " 'seul': 232,\n",
              " 'demain': 229,\n",
              " 'chez': 227,\n",
              " 'ces': 226,\n",
              " 'chien': 224,\n",
              " 'sens': 223,\n",
              " 'toutes': 218,\n",
              " 'mal': 218,\n",
              " 'pensais': 217,\n",
              " 'avait': 215,\n",
              " 'eu': 214,\n",
              " 'aussi': 213,\n",
              " 'assez': 213,\n",
              " 'voulez': 211,\n",
              " 'bonne': 209,\n",
              " 'peur': 209,\n",
              " 'pouvez': 208,\n",
              " 'tes': 207,\n",
              " \"aujourd'hui\": 207,\n",
              " 'devrais': 203,\n",
              " 'partir': 202,\n",
              " 'mieux': 200,\n",
              " \"quelqu'\": 200,\n",
              " 'problème': 197,\n",
              " 'prendre': 196,\n",
              " 'père': 196,\n",
              " 'fut': 195,\n",
              " 'dû': 189,\n",
              " 'aide': 189,\n",
              " 't': 188,\n",
              " 'vos': 187,\n",
              " 'veut': 185,\n",
              " 'soir': 184,\n",
              " 'français': 184,\n",
              " '-t': 184,\n",
              " 'dis': 180,\n",
              " 'sans': 178,\n",
              " 'mère': 176,\n",
              " 'pris': 174,\n",
              " 'vie': 171,\n",
              " 'fois': 170,\n",
              " 'manger': 169,\n",
              " 'enfants': 167,\n",
              " 'homme': 167,\n",
              " 'heure': 167,\n",
              " 'sait': 166,\n",
              " 'parle': 165,\n",
              " 'notre': 162,\n",
              " '-il': 162,\n",
              " 'heureux': 161,\n",
              " 'avais': 160,\n",
              " 'école': 159,\n",
              " 'trois': 158,\n",
              " 'fort': 157,\n",
              " 'raison': 156,\n",
              " 'espère': 156,\n",
              " 'eau': 155,\n",
              " 'ami': 154,\n",
              " 'fille': 154,\n",
              " 'plaît': 154,\n",
              " 'nom': 153,\n",
              " 'boston': 153,\n",
              " 'cet': 153,\n",
              " 'sûr': 152,\n",
              " 'toute': 152,\n",
              " 'adore': 149,\n",
              " 'faites': 147,\n",
              " 'soit': 145,\n",
              " 'savoir': 144,\n",
              " 'soyez': 143,\n",
              " 'amis': 143,\n",
              " 'laisse': 143,\n",
              " 'nuit': 143,\n",
              " 'idée': 142,\n",
              " 'aurais': 141,\n",
              " 'non': 136,\n",
              " 'passé': 136,\n",
              " 'marie': 136,\n",
              " 'venir': 135,\n",
              " 'allons': 135,\n",
              " 'voulais': 135,\n",
              " 'aucun': 134,\n",
              " 'savais': 134,\n",
              " 'crois': 133,\n",
              " 'jour': 132,\n",
              " 'acheté': 132,\n",
              " 'viens': 130,\n",
              " 'allez': 129,\n",
              " 'rester': 128,\n",
              " 'moment': 128,\n",
              " 'heures': 127,\n",
              " 'devons': 127,\n",
              " 'aux': 126,\n",
              " 'vrai': 125,\n",
              " 'chambre': 125,\n",
              " 'semble': 125,\n",
              " 'sera': 124,\n",
              " 'souvent': 124,\n",
              " 'tôt': 124,\n",
              " 'lit': 123,\n",
              " 'vas': 123,\n",
              " 'grand': 122,\n",
              " 'est-il': 122,\n",
              " 'merci': 122,\n",
              " 'devez': 121,\n",
              " 'presque': 121,\n",
              " 'femme': 121,\n",
              " 'ans': 120,\n",
              " 'doit': 120,\n",
              " 'part': 120,\n",
              " 'nouveau': 118,\n",
              " 'sois': 118,\n",
              " 'petit': 117,\n",
              " 'simplement': 116,\n",
              " 'quelques': 116,\n",
              " 'vérité': 116,\n",
              " 'pouvons': 115,\n",
              " 'jouer': 115,\n",
              " 'entendu': 114,\n",
              " 'occupé': 114,\n",
              " 'sœur': 114,\n",
              " 'étiez': 114,\n",
              " 'perdu': 114,\n",
              " 'frère': 113,\n",
              " 'fini': 113,\n",
              " 'déteste': 112,\n",
              " 'ou': 110,\n",
              " 'après': 110,\n",
              " 'seule': 110,\n",
              " 'reste': 109,\n",
              " 'trouve': 109,\n",
              " 'difficile': 108,\n",
              " 'tard': 108,\n",
              " 'arrivé': 108,\n",
              " 'lire': 107,\n",
              " 'mort': 107,\n",
              " 'arrive': 107,\n",
              " 'trouvé': 106,\n",
              " 'laissez': 106,\n",
              " 'pu': 106,\n",
              " 'connais': 106,\n",
              " 'apprécie': 105,\n",
              " 'demandé': 105,\n",
              " 'aucune': 104,\n",
              " 'attendre': 104,\n",
              " 'question': 103,\n",
              " 'allé': 103,\n",
              " 'étaient': 103,\n",
              " 'ignore': 102,\n",
              " 'prêt': 102,\n",
              " 'gens': 101,\n",
              " 'ci': 101,\n",
              " 'main': 101,\n",
              " 'colère': 100,\n",
              " 'pourrais': 100,\n",
              " 'accord': 100,\n",
              " 'café': 100,\n",
              " 'donné': 99,\n",
              " 'jours': 99,\n",
              " 'penses': 99,\n",
              " 'prends': 98,\n",
              " 'prie': 98,\n",
              " 'chance': 98,\n",
              " 'travailler': 97,\n",
              " 'demande': 97,\n",
              " 'nager': 96,\n",
              " 'aimes': 95,\n",
              " 'rendre': 94,\n",
              " 'heureuse': 94,\n",
              " 'dites': 94,\n",
              " 'pourrait': 94,\n",
              " 'bus': 94,\n",
              " 'passe': 93,\n",
              " 'sortir': 93,\n",
              " 'chat': 92,\n",
              " 'vite': 92,\n",
              " 'savez': 92,\n",
              " 'venu': 92,\n",
              " 'veuillez': 91,\n",
              " 'fête': 91,\n",
              " 'parents': 91,\n",
              " 'histoire': 90,\n",
              " 'grande': 90,\n",
              " 'garçon': 89,\n",
              " 'froid': 89,\n",
              " 'regarde': 89,\n",
              " 'entendre': 89,\n",
              " 'donne': 89,\n",
              " 'pièce': 88,\n",
              " 'erreur': 88,\n",
              " 'livres': 88,\n",
              " 'bas': 88,\n",
              " 'avant': 88,\n",
              " 'lettre': 88,\n",
              " 'montre': 87,\n",
              " 'ensemble': 87,\n",
              " 'anglais': 87,\n",
              " 'enfant': 86,\n",
              " 'retard': 86,\n",
              " 'verre': 86,\n",
              " 'serai': 86,\n",
              " 'journée': 86,\n",
              " 'bureau': 85,\n",
              " 'yeux': 85,\n",
              " 'bientôt': 85,\n",
              " 'fatigué': 84,\n",
              " 'tête': 84,\n",
              " 'voudrais': 84,\n",
              " 'arrêter': 83,\n",
              " 'choses': 83,\n",
              " 'téléphone': 83,\n",
              " 'écrit': 83,\n",
              " 'vois': 83,\n",
              " 'vient': 83,\n",
              " 'mot': 82,\n",
              " 'ville': 82,\n",
              " 'leur': 82,\n",
              " 'envie': 82,\n",
              " 'confiance': 82,\n",
              " 'arrête': 82,\n",
              " 'voulait': 82,\n",
              " 'aimez': 81,\n",
              " 'boire': 81,\n",
              " 'matin': 81,\n",
              " 'désolé': 81,\n",
              " 'plan': 81,\n",
              " 'mauvais': 81,\n",
              " 'plutôt': 80,\n",
              " 'essayé': 80,\n",
              " 'sujet': 79,\n",
              " 'jeune': 79,\n",
              " 'celui': 79,\n",
              " 'faim': 79,\n",
              " 'vit': 79,\n",
              " 'parlé': 79,\n",
              " 'vieux': 78,\n",
              " 'famille': 78,\n",
              " \"jusqu'\": 77,\n",
              " 'devriez': 77,\n",
              " 'coup': 77,\n",
              " 'laissé': 77,\n",
              " 'ainsi': 77,\n",
              " 'film': 77,\n",
              " 'âge': 76,\n",
              " 'dur': 76,\n",
              " 'amie': 76,\n",
              " 'essayer': 76,\n",
              " 'déjeuner': 76,\n",
              " 'réponse': 76,\n",
              " 'tellement': 76,\n",
              " 'mangé': 75,\n",
              " 'depuis': 75,\n",
              " 'davantage': 75,\n",
              " 'comprends': 74,\n",
              " 'mis': 74,\n",
              " 'vélo': 74,\n",
              " 'vin': 74,\n",
              " 'place': 74,\n",
              " 'photo': 74,\n",
              " 'partie': 74,\n",
              " 'trouver': 73,\n",
              " 'bière': 73,\n",
              " 'vivre': 73,\n",
              " 'secret': 73,\n",
              " 'loin': 73,\n",
              " 'médecin': 73,\n",
              " 'longtemps': 73,\n",
              " 'travaille': 73,\n",
              " 'oublié': 73,\n",
              " 'musique': 73,\n",
              " 'choix': 73,\n",
              " 'appris': 72,\n",
              " 'malade': 72,\n",
              " 'belle': 72,\n",
              " 'attention': 71,\n",
              " 'feu': 71,\n",
              " 'prenez': 71,\n",
              " 'dormir': 71,\n",
              " 'acheter': 71,\n",
              " 'dîner': 70,\n",
              " 'facile': 70,\n",
              " 'chaud': 70,\n",
              " 'meilleur': 69,\n",
              " 'gros': 69,\n",
              " 'dessus': 69,\n",
              " 'fils': 69,\n",
              " 'boulot': 69,\n",
              " 'thé': 68,\n",
              " 'réunion': 67,\n",
              " 'sous': 67,\n",
              " 'conduire': 66,\n",
              " 'pensé': 66,\n",
              " 'étions': 66,\n",
              " 'joue': 66,\n",
              " 'plait': 66,\n",
              " 'beau': 65,\n",
              " 'lu': 65,\n",
              " 'compte': 65,\n",
              " 'bois': 65,\n",
              " 'décision': 65,\n",
              " 'devrions': 64,\n",
              " 'ferai': 64,\n",
              " 'entre': 64,\n",
              " 'nouvelle': 64,\n",
              " 'attends': 64,\n",
              " 'arrêté': 64,\n",
              " 'chanter': 64,\n",
              " 'tort': 63,\n",
              " 'ferais': 63,\n",
              " 'pensez': 63,\n",
              " 'sûre': 63,\n",
              " 'appeler': 62,\n",
              " 'fenêtre': 62,\n",
              " 'semaine': 62,\n",
              " 'cœur': 62,\n",
              " 'prête': 61,\n",
              " 'regarder': 61,\n",
              " 'table': 61,\n",
              " 'vue': 61,\n",
              " 'chaque': 60,\n",
              " 'donner': 60,\n",
              " 'fumer': 60,\n",
              " 'terminé': 60,\n",
              " 'prix': 60,\n",
              " 'mange': 59,\n",
              " 'écrire': 59,\n",
              " 'agit': 59,\n",
              " 'possible': 59,\n",
              " 'chapeau': 59,\n",
              " 'serait': 59,\n",
              " 'passer': 58,\n",
              " 'appelle': 58,\n",
              " 'dehors': 58,\n",
              " 'mais': 58,\n",
              " 'petite': 58,\n",
              " 'bébé': 58,\n",
              " 'décidé': 58,\n",
              " 'endroit': 58,\n",
              " 'premier': 57,\n",
              " 'riche': 57,\n",
              " 'robe': 57,\n",
              " 'dernière': 57,\n",
              " 'parti': 57,\n",
              " 'cheveux': 57,\n",
              " 'restez': 57,\n",
              " 'nos': 57,\n",
              " 'vis': 57,\n",
              " 'pouvoir': 57,\n",
              " 'surpris': 57,\n",
              " 'voulons': 56,\n",
              " 'faisons': 56,\n",
              " 'parlez': 56,\n",
              " 'occupée': 56,\n",
              " 'immédiatement': 56,\n",
              " 'sécurité': 55,\n",
              " 'filles': 55,\n",
              " 'quiconque': 55,\n",
              " 'rapidement': 55,\n",
              " 'autres': 55,\n",
              " 'payer': 54,\n",
              " 'arbre': 54,\n",
              " 'dix': 54,\n",
              " 'mourir': 54,\n",
              " 'aurait': 54,\n",
              " 'arriver': 54,\n",
              " 'tant': 54,\n",
              " 'devrait': 54,\n",
              " 'lundi': 54,\n",
              " 'plein': 53,\n",
              " 'amies': 53,\n",
              " 'menti': 53,\n",
              " 'aimé': 53,\n",
              " 'gare': 53,\n",
              " 'clé': 53,\n",
              " 'rappelle': 53,\n",
              " 'fou': 53,\n",
              " 'sac': 52,\n",
              " 'nourriture': 52,\n",
              " 'exactement': 52,\n",
              " 'probablement': 52,\n",
              " 'voici': 52,\n",
              " 'croire': 52,\n",
              " 'noël': 52,\n",
              " 'perdre': 52,\n",
              " 'lait': 51,\n",
              " 'près': 51,\n",
              " 'voyage': 51,\n",
              " 'eux': 51,\n",
              " 'chaussures': 51,\n",
              " 'donnez': 51,\n",
              " 'mets': 51,\n",
              " 'préfère': 50,\n",
              " 'moins': 50,\n",
              " 'pendant': 50,\n",
              " 'commencer': 50,\n",
              " 'gâteau': 50,\n",
              " 'reçu': 50,\n",
              " 'demander': 50,\n",
              " 'questions': 50,\n",
              " 'sérieux': 50,\n",
              " 'essaie': 49,\n",
              " 'point': 49,\n",
              " 'peuvent': 49,\n",
              " 'rire': 49,\n",
              " 'veulent': 49,\n",
              " 'savait': 49,\n",
              " 'gagner': 49,\n",
              " 'important': 49,\n",
              " 'type': 49,\n",
              " 'commence': 49,\n",
              " 'auparavant': 49,\n",
              " 'minutes': 48,\n",
              " 'changé': 48,\n",
              " 'né': 48,\n",
              " '«': 48,\n",
              " '»': 48,\n",
              " 'mois': 48,\n",
              " 'contre': 48,\n",
              " 'apprendre': 48,\n",
              " 'minute': 48,\n",
              " 'fier': 48,\n",
              " 'complètement': 48,\n",
              " 'œil': 48,\n",
              " 'rencontrer': 48,\n",
              " 'affaire': 48,\n",
              " 'commencé': 47,\n",
              " 'blessé': 47,\n",
              " 'australie': 47,\n",
              " 'tennis': 47,\n",
              " 'bruit': 47,\n",
              " 'revoir': 47,\n",
              " 'côté': 47,\n",
              " 'devoirs': 47,\n",
              " 'santé': 46,\n",
              " 'clés': 46,\n",
              " 'rencontré': 46,\n",
              " 'parc': 46,\n",
              " 'équipe': 46,\n",
              " 'désolée': 46,\n",
              " 'serais': 46,\n",
              " 'étudier': 46,\n",
              " 'volé': 46,\n",
              " 'chiens': 46,\n",
              " 'professeur': 46,\n",
              " 'affaires': 46,\n",
              " 'mettre': 45,\n",
              " 'police': 45,\n",
              " 'entrer': 45,\n",
              " 'année': 45,\n",
              " 'derrière': 45,\n",
              " 'seulement': 45,\n",
              " 'stupide': 45,\n",
              " 'devenir': 45,\n",
              " 'arrêtez': 45,\n",
              " 'celle': 45,\n",
              " 'pourriez': 45,\n",
              " 'vont': 44,\n",
              " 'laisser': 44,\n",
              " 'tasse': 44,\n",
              " 'gentil': 44,\n",
              " 'dispose': 44,\n",
              " 'font': 44,\n",
              " 'drôle': 44,\n",
              " 'discuter': 44,\n",
              " 'mains': 44,\n",
              " 'amour': 44,\n",
              " 'danser': 44,\n",
              " 'bizarre': 44,\n",
              " 'lieu': 44,\n",
              " 'suppose': 44,\n",
              " 'poisson': 44,\n",
              " 'danger': 44,\n",
              " 'désormais': 44,\n",
              " 'alors': 44,\n",
              " 'genre': 44,\n",
              " 'chercher': 43,\n",
              " 'dernier': 43,\n",
              " 'viendra': 43,\n",
              " 'cinq': 43,\n",
              " 'marché': 43,\n",
              " 'venez': 43,\n",
              " 'rend': 43,\n",
              " 'parfait': 43,\n",
              " 'conseil': 43,\n",
              " 'faute': 43,\n",
              " 'boîte': 43,\n",
              " 'manteau': 43,\n",
              " 'amusant': 43,\n",
              " 'nouvelles': 43,\n",
              " 'utiliser': 43,\n",
              " 'surprise': 42,\n",
              " 'fit': 42,\n",
              " 'étranger': 42,\n",
              " 'propre': 42,\n",
              " 'retour': 42,\n",
              " 'rouge': 42,\n",
              " 'regardez': 42,\n",
              " 'chanson': 42,\n",
              " 'peine': 42,\n",
              " 'appelé': 42,\n",
              " 'avion': 42,\n",
              " 'pleurer': 42,\n",
              " 'cassé': 42,\n",
              " 'numéro': 42,\n",
              " 'bu': 41,\n",
              " 'chemin': 41,\n",
              " 'dormi': 41,\n",
              " 'payé': 41,\n",
              " 'plaisir': 41,\n",
              " 'c’': 41,\n",
              " 'garder': 41,\n",
              " 'seconde': 41,\n",
              " 'soin': 41,\n",
              " 'importe': 41,\n",
              " 'garde': 41,\n",
              " 'coûte': 41,\n",
              " 'j’': 41,\n",
              " 'cours': 41,\n",
              " 'cheval': 40,\n",
              " 'changer': 40,\n",
              " 'entends': 40,\n",
              " 'vêtements': 40,\n",
              " 'plage': 40,\n",
              " 'ordinateur': 40,\n",
              " 'langue': 40,\n",
              " 'tomber': 40,\n",
              " 'mari': 40,\n",
              " 'rêve': 40,\n",
              " 'marié': 39,\n",
              " 'télé': 39,\n",
              " 'content': 39,\n",
              " 'intéressant': 39,\n",
              " 'emploi': 39,\n",
              " 'rendu': 39,\n",
              " 'marcher': 39,\n",
              " 'lumière': 39,\n",
              " 'vers': 39,\n",
              " 'irai': 39,\n",
              " 'cuisine': 39,\n",
              " 'mauvaise': 39,\n",
              " 'bateau': 38,\n",
              " 'sorti': 38,\n",
              " 'accident': 38,\n",
              " 'préféré': 38,\n",
              " 'souviens': 38,\n",
              " 'assis': 38,\n",
              " 'ouvrir': 38,\n",
              " 'stylo': 38,\n",
              " 'court': 38,\n",
              " 'voulu': 38,\n",
              " 'oublie': 38,\n",
              " 'piano': 38,\n",
              " 'dont': 38,\n",
              " 'problèmes': 38,\n",
              " 'pays': 38,\n",
              " 'étudie': 38,\n",
              " 'fera': 38,\n",
              " 'bras': 38,\n",
              " 'soleil': 38,\n",
              " 'hôtel': 37,\n",
              " 'erreurs': 37,\n",
              " 'voudriez': 37,\n",
              " 'simple': 37,\n",
              " 'dangereux': 37,\n",
              " 'semblez': 37,\n",
              " 'libre': 37,\n",
              " 'pleuvoir': 37,\n",
              " 'répondre': 37,\n",
              " 'l’': 37,\n",
              " 'triste': 37,\n",
              " 'autant': 37,\n",
              " 'compris': 37,\n",
              " 'vacances': 37,\n",
              " 'jeu': 37,\n",
              " 'situation': 37,\n",
              " 'adresse': 37,\n",
              " 'chemise': 36,\n",
              " 'suivre': 36,\n",
              " 'hors': 36,\n",
              " 'examen': 36,\n",
              " 'pommes': 36,\n",
              " 'espoir': 36,\n",
              " 'magasin': 36,\n",
              " 'trente': 36,\n",
              " 'connaît': 36,\n",
              " 'rends': 35,\n",
              " 'mit': 35,\n",
              " 'produit': 35,\n",
              " 'vaut': 35,\n",
              " 'écoute': 35,\n",
              " 'courir': 35,\n",
              " 'parles': 35,\n",
              " 'réparer': 35,\n",
              " 'nécessaire': 35,\n",
              " 'fermé': 35,\n",
              " 'vol': 35,\n",
              " 'pars': 35,\n",
              " 'souhaite': 35,\n",
              " 'devenu': 35,\n",
              " 'lunettes': 34,\n",
              " 'mots': 34,\n",
              " 'fleurs': 34,\n",
              " 'voyager': 34,\n",
              " 'journal': 34,\n",
              " 'étudiant': 34,\n",
              " 'mentir': 34,\n",
              " 'habite': 34,\n",
              " 'est-elle': 34,\n",
              " 'porter': 34,\n",
              " 'savons': 34,\n",
              " 'tableau': 34,\n",
              " 'prit': 34,\n",
              " 'liste': 34,\n",
              " 'certain': 34,\n",
              " 'asseoir': 34,\n",
              " 'pire': 34,\n",
              " 'rentrer': 34,\n",
              " 'allée': 34,\n",
              " 'comprendre': 34,\n",
              " 'poids': 34,\n",
              " 'également': 33,\n",
              " 'avis': 33,\n",
              " 'cher': 33,\n",
              " 'calme': 33,\n",
              " 'neige': 33,\n",
              " 'pied': 33,\n",
              " 'regardé': 33,\n",
              " 'occuper': 33,\n",
              " 'instant': 33,\n",
              " 'prend': 33,\n",
              " 'noir': 33,\n",
              " 'succès': 33,\n",
              " 'six': 33,\n",
              " 'carte': 33,\n",
              " 'suite': 33,\n",
              " 'hôpital': 33,\n",
              " 'gagné': 33,\n",
              " 'serez': 33,\n",
              " 'meilleure': 33,\n",
              " 'ouvert': 33,\n",
              " 'esprit': 32,\n",
              " 'auriez': 32,\n",
              " 'voilà': 32,\n",
              " 'inquiète': 32,\n",
              " 'pouvais': 32,\n",
              " 'ciel': 32,\n",
              " 'façon': 32,\n",
              " 'route': 32,\n",
              " 'droit': 32,\n",
              " 'idiot': 32,\n",
              " 'ferme': 32,\n",
              " 'prudent': 32,\n",
              " 'ignorais': 32,\n",
              " 'long': 32,\n",
              " 'dictionnaire': 32,\n",
              " 'après-midi': 32,\n",
              " 'sorte': 32,\n",
              " 'devant': 32,\n",
              " 'aidé': 32,\n",
              " 'rue': 32,\n",
              " 'pieds': 32,\n",
              " 'honte': 31,\n",
              " 'mur': 31,\n",
              " 'apporté': 31,\n",
              " 'règles': 31,\n",
              " 'montré': 31,\n",
              " 'essayez': 31,\n",
              " 'n’': 31,\n",
              " 'prêts': 31,\n",
              " 'montrer': 31,\n",
              " 'projet': 31,\n",
              " 'oncle': 31,\n",
              " 'attendu': 31,\n",
              " 'seuls': 31,\n",
              " 'cadeau': 31,\n",
              " 'pleut': 31,\n",
              " 'tuer': 31,\n",
              " 'visite': 31,\n",
              " 'excuses': 31,\n",
              " 'eut': 31,\n",
              " 'pomme': 31,\n",
              " 'amoureux': 30,\n",
              " 'cherche': 30,\n",
              " 'lac': 30,\n",
              " 'canapé': 30,\n",
              " 'grosse': 30,\n",
              " 'dimanche': 30,\n",
              " 'vieille': 30,\n",
              " 'avocat': 30,\n",
              " 'arme': 30,\n",
              " 'jardin': 30,\n",
              " '30': 30,\n",
              " 'midi': 30,\n",
              " 'quelles': 30,\n",
              " 'message': 30,\n",
              " 'voix': 30,\n",
              " 'dents': 30,\n",
              " 'faisait': 30,\n",
              " 'fasse': 29,\n",
              " 'étudiants': 29,\n",
              " 'idées': 29,\n",
              " 'vide': 29,\n",
              " 'invité': 29,\n",
              " 'avaient': 29,\n",
              " 'offre': 29,\n",
              " 'fin': 29,\n",
              " 'classe': 29,\n",
              " 'lis': 29,\n",
              " 'note': 29,\n",
              " 'attendez': 29,\n",
              " 'tué': 29,\n",
              " 'avance': 29,\n",
              " 'dirai': 29,\n",
              " 'lever': 29,\n",
              " 'repas': 29,\n",
              " 'prison': 29,\n",
              " 'portes': 29,\n",
              " 'gérer': 29,\n",
              " 'penser': 29,\n",
              " 'pluie': 29,\n",
              " 'poser': 29,\n",
              " 'poste': 29,\n",
              " 'réussi': 29,\n",
              " 'guerre': 29,\n",
              " 'retourner': 29,\n",
              " 'ceux': 29,\n",
              " 'silencieux': 28,\n",
              " 'utile': 28,\n",
              " 'sommeil': 28,\n",
              " 'demanda': 28,\n",
              " 'parapluie': 28,\n",
              " 'expérience': 28,\n",
              " 'mariée': 28,\n",
              " 'pain': 28,\n",
              " 'papier': 28,\n",
              " 'sortez': 28,\n",
              " 'chef': 28,\n",
              " 'proposition': 28,\n",
              " 'tenir': 28,\n",
              " 'parfaitement': 28,\n",
              " 'rarement': 28,\n",
              " 'œufs': 28,\n",
              " 'tombé': 28,\n",
              " 'cravate': 28,\n",
              " 'jolie': 28,\n",
              " 'tranquille': 28,\n",
              " 'rapport': 28,\n",
              " 'bons': 28,\n",
              " 'japon': 28,\n",
              " 'appareil': 28,\n",
              " 'dos': 28,\n",
              " 'gagne': 28,\n",
              " 'endormi': 28,\n",
              " 'opinion': 28,\n",
              " 'dérange': 28,\n",
              " 'dieu': 28,\n",
              " 'chante': 28,\n",
              " 'fatiguée': 27,\n",
              " 'responsable': 27,\n",
              " 'étrange': 27,\n",
              " 'tour': 27,\n",
              " 'manque': 27,\n",
              " 'aiment': 27,\n",
              " 'bonnes': 27,\n",
              " 'terre': 27,\n",
              " 'fonctionne': 27,\n",
              " 'marche': 27,\n",
              " 'chaise': 27,\n",
              " 'jambe': 27,\n",
              " 'sors': 27,\n",
              " 'senti': 27,\n",
              " 'quels': 27,\n",
              " 'écouter': 27,\n",
              " 'nez': 27,\n",
              " 'promis': 27,\n",
              " 'blanc': 27,\n",
              " 'emprunter': 27,\n",
              " 'fermer': 27,\n",
              " 'parole': 27,\n",
              " 'week': 27,\n",
              " 'pouvait': 27,\n",
              " 'prochain': 27,\n",
              " 'nombreux': 27,\n",
              " 'fus': 27,\n",
              " 'goût': 27,\n",
              " 'mien': 27,\n",
              " 'marier': 27,\n",
              " 'joué': 27,\n",
              " 'venue': 26,\n",
              " 'tiens': 26,\n",
              " 'partout': 26,\n",
              " 'forte': 26,\n",
              " 'capable': 26,\n",
              " 'aviez': 26,\n",
              " 'rivière': 26,\n",
              " 'furent': 26,\n",
              " 'devoir': 26,\n",
              " 'portait': 26,\n",
              " 'suffisamment': 26,\n",
              " 'télévision': 26,\n",
              " 'seras': 26,\n",
              " 'honnête': 26,\n",
              " 'conseils': 26,\n",
              " 'finir': 26,\n",
              " 'menteur': 26,\n",
              " 'différent': 26,\n",
              " 'laquelle': 26,\n",
              " 'end': 26,\n",
              " 'ouverte': 26,\n",
              " 'faveur': 26,\n",
              " 'résoudre': 26,\n",
              " 'dedans': 26,\n",
              " 'attend': 26,\n",
              " 'oui': 25,\n",
              " 'restaurant': 25,\n",
              " 'essaye': 25,\n",
              " 'parfois': 25,\n",
              " 'reviens': 25,\n",
              " 'couleur': 25,\n",
              " 'patron': 25,\n",
              " 'aie': 25,\n",
              " 'préférée': 25,\n",
              " 'frères': 25,\n",
              " 'aise': 25,\n",
              " 'ouvrez': 25,\n",
              " 'blague': 25,\n",
              " 'paix': 25,\n",
              " 'douche': 25,\n",
              " 'manqué': 25,\n",
              " 'radio': 25,\n",
              " 'sucre': 25,\n",
              " 'chats': 25,\n",
              " 'lève': 25,\n",
              " 'taille': 25,\n",
              " 'connaissez': 25,\n",
              " 'bord': 25,\n",
              " 'battre': 25,\n",
              " 'allait': 25,\n",
              " 'coin': 25,\n",
              " 'propres': 25,\n",
              " 'aimeriez': 24,\n",
              " 'risque': 24,\n",
              " ...}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ujDUnWmJBLgz"
      },
      "source": [
        "#### 5. Create a new dataframe *sample_filt* containing only sentence pairs from *sample_df* where all English tokens are in en_common, and all French tokens are in fr_common. Also only use pairs where both sentences contain at most *maxlen* tokens. How many sentence pairs are in the filtered data?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-24T11:36:14.551542Z",
          "start_time": "2020-06-24T11:36:14.497714Z"
        },
        "id": "ie98dGvbszMb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "e4453f7d-4605-40fe-86fe-e5c9ebe45895"
      },
      "source": [
        "maxlen = 20\n",
        "\n",
        "sample_filt =sample_df[(sample_df['en_tokens'].str.len()  <= maxlen) & (sample_df['fr_tokens'].str.len() <= maxlen)]\n",
        "sample_filt"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>en</th>\n",
              "      <th>fr</th>\n",
              "      <th>en_tokens</th>\n",
              "      <th>fr_tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What did I do wrong?</td>\n",
              "      <td>Qu'ai-je fait de travers ?</td>\n",
              "      <td>[&lt;start&gt;, what, did, i, do, wrong, ?, &lt;end&gt;]</td>\n",
              "      <td>[&lt;start&gt;, qu', ai, -, je, fait, de, travers, ?, &lt;end&gt;]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>You like olives, don't you?</td>\n",
              "      <td>Vous aimez les olives, pas vrai ?</td>\n",
              "      <td>[&lt;start&gt;, you, like, olives, ,, do, n't, you, ?, &lt;end&gt;]</td>\n",
              "      <td>[&lt;start&gt;, vous, aimez, les, olives, ,, pas, vrai, ?, &lt;end&gt;]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I really appreciate your coming.</td>\n",
              "      <td>J'apprécie vraiment ta venue.</td>\n",
              "      <td>[&lt;start&gt;, i, really, appreciate, your, coming, ., &lt;end&gt;]</td>\n",
              "      <td>[&lt;start&gt;, j', apprécie, vraiment, ta, venue, ., &lt;end&gt;]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I had to do something.</td>\n",
              "      <td>Il fallait que je fasse quelque chose.</td>\n",
              "      <td>[&lt;start&gt;, i, had, to, do, something, ., &lt;end&gt;]</td>\n",
              "      <td>[&lt;start&gt;, il, fallait, que, je, fasse, quelque, chose, ., &lt;end&gt;]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I know Tom was first.</td>\n",
              "      <td>Je sais que Tom était le premier.</td>\n",
              "      <td>[&lt;start&gt;, i, know, tom, was, first, ., &lt;end&gt;]</td>\n",
              "      <td>[&lt;start&gt;, je, sais, que, tom, était, le, premier, ., &lt;end&gt;]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49995</th>\n",
              "      <td>He borrowed the car from his friend.</td>\n",
              "      <td>Il a emprunté la voiture à un ami.</td>\n",
              "      <td>[&lt;start&gt;, he, borrowed, the, car, from, his, friend, ., &lt;end&gt;]</td>\n",
              "      <td>[&lt;start&gt;, il, a, emprunté, la, voiture, à, un, ami, ., &lt;end&gt;]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49996</th>\n",
              "      <td>Send us a message.</td>\n",
              "      <td>Envoie-nous un message.</td>\n",
              "      <td>[&lt;start&gt;, send, us, a, message, ., &lt;end&gt;]</td>\n",
              "      <td>[&lt;start&gt;, envoie, -, nous, un, message, ., &lt;end&gt;]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49997</th>\n",
              "      <td>Who's coming for dinner?</td>\n",
              "      <td>Qui vient dîner ?</td>\n",
              "      <td>[&lt;start&gt;, who, 's, coming, for, dinner, ?, &lt;end&gt;]</td>\n",
              "      <td>[&lt;start&gt;, qui, vient, dîner, ?, &lt;end&gt;]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49998</th>\n",
              "      <td>You're lying!</td>\n",
              "      <td>Vous mentez !</td>\n",
              "      <td>[&lt;start&gt;, you, 're, lying, !, &lt;end&gt;]</td>\n",
              "      <td>[&lt;start&gt;, vous, mentez, !, &lt;end&gt;]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49999</th>\n",
              "      <td>I always thought Tom was funny.</td>\n",
              "      <td>J'ai toujours pensé que Tom était drôle.</td>\n",
              "      <td>[&lt;start&gt;, i, always, thought, tom, was, funny, ., &lt;end&gt;]</td>\n",
              "      <td>[&lt;start&gt;, j', ai, toujours, pensé, que, tom, était, drôle, ., &lt;end&gt;]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>49999 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         en  ...                                                             fr_tokens\n",
              "0                      What did I do wrong?  ...                [<start>, qu', ai, -, je, fait, de, travers, ?, <end>]\n",
              "1               You like olives, don't you?  ...           [<start>, vous, aimez, les, olives, ,, pas, vrai, ?, <end>]\n",
              "2          I really appreciate your coming.  ...                [<start>, j', apprécie, vraiment, ta, venue, ., <end>]\n",
              "3                    I had to do something.  ...      [<start>, il, fallait, que, je, fasse, quelque, chose, ., <end>]\n",
              "4                     I know Tom was first.  ...           [<start>, je, sais, que, tom, était, le, premier, ., <end>]\n",
              "...                                     ...  ...                                                                   ...\n",
              "49995  He borrowed the car from his friend.  ...         [<start>, il, a, emprunté, la, voiture, à, un, ami, ., <end>]\n",
              "49996                    Send us a message.  ...                     [<start>, envoie, -, nous, un, message, ., <end>]\n",
              "49997              Who's coming for dinner?  ...                                [<start>, qui, vient, dîner, ?, <end>]\n",
              "49998                         You're lying!  ...                                     [<start>, vous, mentez, !, <end>]\n",
              "49999       I always thought Tom was funny.  ...  [<start>, j', ai, toujours, pensé, que, tom, était, drôle, ., <end>]\n",
              "\n",
              "[49999 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-24T11:36:15.682561Z",
          "start_time": "2020-06-24T11:36:15.000462Z"
        },
        "colab_type": "code",
        "id": "LT2kozSM_koK",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "outputId": "6a6c4329-6f27-474d-a031-4d34d6b02e68"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "sample_filt = sample_filt[(sample_filt['en_tokens'].apply(lambda a: \n",
        "                                                      (np.all([i in en_common.keys() for i in a])))) &\n",
        "         (sample_filt['fr_tokens'].apply(lambda a: \n",
        "                                       np.all([i in fr_common.keys() for i in a])))]\n",
        "sample_filt"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>en</th>\n",
              "      <th>fr</th>\n",
              "      <th>en_tokens</th>\n",
              "      <th>fr_tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What did I do wrong?</td>\n",
              "      <td>Qu'ai-je fait de travers ?</td>\n",
              "      <td>[&lt;start&gt;, what, did, i, do, wrong, ?, &lt;end&gt;]</td>\n",
              "      <td>[&lt;start&gt;, qu', ai, -, je, fait, de, travers, ?, &lt;end&gt;]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I really appreciate your coming.</td>\n",
              "      <td>J'apprécie vraiment ta venue.</td>\n",
              "      <td>[&lt;start&gt;, i, really, appreciate, your, coming, ., &lt;end&gt;]</td>\n",
              "      <td>[&lt;start&gt;, j', apprécie, vraiment, ta, venue, ., &lt;end&gt;]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I had to do something.</td>\n",
              "      <td>Il fallait que je fasse quelque chose.</td>\n",
              "      <td>[&lt;start&gt;, i, had, to, do, something, ., &lt;end&gt;]</td>\n",
              "      <td>[&lt;start&gt;, il, fallait, que, je, fasse, quelque, chose, ., &lt;end&gt;]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I know Tom was first.</td>\n",
              "      <td>Je sais que Tom était le premier.</td>\n",
              "      <td>[&lt;start&gt;, i, know, tom, was, first, ., &lt;end&gt;]</td>\n",
              "      <td>[&lt;start&gt;, je, sais, que, tom, était, le, premier, ., &lt;end&gt;]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>They all drank.</td>\n",
              "      <td>Elles ont toutes bu.</td>\n",
              "      <td>[&lt;start&gt;, they, all, drank, ., &lt;end&gt;]</td>\n",
              "      <td>[&lt;start&gt;, elles, ont, toutes, bu, ., &lt;end&gt;]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49993</th>\n",
              "      <td>That boy showed no fear.</td>\n",
              "      <td>Ce garçon ne montra aucune peur.</td>\n",
              "      <td>[&lt;start&gt;, that, boy, showed, no, fear, ., &lt;end&gt;]</td>\n",
              "      <td>[&lt;start&gt;, ce, garçon, ne, montra, aucune, peur, ., &lt;end&gt;]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49994</th>\n",
              "      <td>I couldn't stand it any longer.</td>\n",
              "      <td>Je ne pourrais davantage le supporter.</td>\n",
              "      <td>[&lt;start&gt;, i, could, n't, stand, it, any, longer, ., &lt;end&gt;]</td>\n",
              "      <td>[&lt;start&gt;, je, ne, pourrais, davantage, le, supporter, ., &lt;end&gt;]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49995</th>\n",
              "      <td>He borrowed the car from his friend.</td>\n",
              "      <td>Il a emprunté la voiture à un ami.</td>\n",
              "      <td>[&lt;start&gt;, he, borrowed, the, car, from, his, friend, ., &lt;end&gt;]</td>\n",
              "      <td>[&lt;start&gt;, il, a, emprunté, la, voiture, à, un, ami, ., &lt;end&gt;]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49997</th>\n",
              "      <td>Who's coming for dinner?</td>\n",
              "      <td>Qui vient dîner ?</td>\n",
              "      <td>[&lt;start&gt;, who, 's, coming, for, dinner, ?, &lt;end&gt;]</td>\n",
              "      <td>[&lt;start&gt;, qui, vient, dîner, ?, &lt;end&gt;]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49999</th>\n",
              "      <td>I always thought Tom was funny.</td>\n",
              "      <td>J'ai toujours pensé que Tom était drôle.</td>\n",
              "      <td>[&lt;start&gt;, i, always, thought, tom, was, funny, ., &lt;end&gt;]</td>\n",
              "      <td>[&lt;start&gt;, j', ai, toujours, pensé, que, tom, était, drôle, ., &lt;end&gt;]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>30815 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         en  ...                                                             fr_tokens\n",
              "0                      What did I do wrong?  ...                [<start>, qu', ai, -, je, fait, de, travers, ?, <end>]\n",
              "2          I really appreciate your coming.  ...                [<start>, j', apprécie, vraiment, ta, venue, ., <end>]\n",
              "3                    I had to do something.  ...      [<start>, il, fallait, que, je, fasse, quelque, chose, ., <end>]\n",
              "4                     I know Tom was first.  ...           [<start>, je, sais, que, tom, était, le, premier, ., <end>]\n",
              "5                           They all drank.  ...                           [<start>, elles, ont, toutes, bu, ., <end>]\n",
              "...                                     ...  ...                                                                   ...\n",
              "49993              That boy showed no fear.  ...             [<start>, ce, garçon, ne, montra, aucune, peur, ., <end>]\n",
              "49994       I couldn't stand it any longer.  ...       [<start>, je, ne, pourrais, davantage, le, supporter, ., <end>]\n",
              "49995  He borrowed the car from his friend.  ...         [<start>, il, a, emprunté, la, voiture, à, un, ami, ., <end>]\n",
              "49997              Who's coming for dinner?  ...                                [<start>, qui, vient, dîner, ?, <end>]\n",
              "49999       I always thought Tom was funny.  ...  [<start>, j', ai, toujours, pensé, que, tom, était, drôle, ., <end>]\n",
              "\n",
              "[30815 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8mAvlO8MBLd6"
      },
      "source": [
        "#### 6. For convenience we want to work with strings instead of arrays of tokens. Create new columns *en_txt* and *fr_txt* in the dataframe *sample_filt* containing the tokens in a sentence separated by spaces. For example, the column *fr_txt* should include the string \"&lt;start&gt; ferme - la juste et écoute ! &lt;end&gt;\" in some row.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-24T11:36:27.955560Z",
          "start_time": "2020-06-24T11:36:27.879680Z"
        },
        "colab_type": "code",
        "id": "pmETxpr__kuU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "outputId": "c7c40c8b-442d-497f-b4d5-addcce81178f"
      },
      "source": [
        "import pandas as pd\n",
        "pd.options.mode.chained_assignment = None  # default='warn'\n",
        "\n",
        "sample_filt['en_txt'] = sample_filt.en_tokens.apply(lambda a: ' '.join(a))\n",
        "sample_filt['fr_txt'] = sample_filt.fr_tokens.apply(lambda a: ' '.join(a))\n",
        "sample_filt"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>en</th>\n",
              "      <th>fr</th>\n",
              "      <th>en_tokens</th>\n",
              "      <th>fr_tokens</th>\n",
              "      <th>en_txt</th>\n",
              "      <th>fr_txt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What did I do wrong?</td>\n",
              "      <td>Qu'ai-je fait de travers ?</td>\n",
              "      <td>[&lt;start&gt;, what, did, i, do, wrong, ?, &lt;end&gt;]</td>\n",
              "      <td>[&lt;start&gt;, qu', ai, -, je, fait, de, travers, ?, &lt;end&gt;]</td>\n",
              "      <td>&lt;start&gt; what did i do wrong ? &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; qu' ai - je fait de travers ? &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I really appreciate your coming.</td>\n",
              "      <td>J'apprécie vraiment ta venue.</td>\n",
              "      <td>[&lt;start&gt;, i, really, appreciate, your, coming, ., &lt;end&gt;]</td>\n",
              "      <td>[&lt;start&gt;, j', apprécie, vraiment, ta, venue, ., &lt;end&gt;]</td>\n",
              "      <td>&lt;start&gt; i really appreciate your coming . &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; j' apprécie vraiment ta venue . &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I had to do something.</td>\n",
              "      <td>Il fallait que je fasse quelque chose.</td>\n",
              "      <td>[&lt;start&gt;, i, had, to, do, something, ., &lt;end&gt;]</td>\n",
              "      <td>[&lt;start&gt;, il, fallait, que, je, fasse, quelque, chose, ., &lt;end&gt;]</td>\n",
              "      <td>&lt;start&gt; i had to do something . &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; il fallait que je fasse quelque chose . &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I know Tom was first.</td>\n",
              "      <td>Je sais que Tom était le premier.</td>\n",
              "      <td>[&lt;start&gt;, i, know, tom, was, first, ., &lt;end&gt;]</td>\n",
              "      <td>[&lt;start&gt;, je, sais, que, tom, était, le, premier, ., &lt;end&gt;]</td>\n",
              "      <td>&lt;start&gt; i know tom was first . &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; je sais que tom était le premier . &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>They all drank.</td>\n",
              "      <td>Elles ont toutes bu.</td>\n",
              "      <td>[&lt;start&gt;, they, all, drank, ., &lt;end&gt;]</td>\n",
              "      <td>[&lt;start&gt;, elles, ont, toutes, bu, ., &lt;end&gt;]</td>\n",
              "      <td>&lt;start&gt; they all drank . &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; elles ont toutes bu . &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49993</th>\n",
              "      <td>That boy showed no fear.</td>\n",
              "      <td>Ce garçon ne montra aucune peur.</td>\n",
              "      <td>[&lt;start&gt;, that, boy, showed, no, fear, ., &lt;end&gt;]</td>\n",
              "      <td>[&lt;start&gt;, ce, garçon, ne, montra, aucune, peur, ., &lt;end&gt;]</td>\n",
              "      <td>&lt;start&gt; that boy showed no fear . &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; ce garçon ne montra aucune peur . &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49994</th>\n",
              "      <td>I couldn't stand it any longer.</td>\n",
              "      <td>Je ne pourrais davantage le supporter.</td>\n",
              "      <td>[&lt;start&gt;, i, could, n't, stand, it, any, longer, ., &lt;end&gt;]</td>\n",
              "      <td>[&lt;start&gt;, je, ne, pourrais, davantage, le, supporter, ., &lt;end&gt;]</td>\n",
              "      <td>&lt;start&gt; i could n't stand it any longer . &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; je ne pourrais davantage le supporter . &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49995</th>\n",
              "      <td>He borrowed the car from his friend.</td>\n",
              "      <td>Il a emprunté la voiture à un ami.</td>\n",
              "      <td>[&lt;start&gt;, he, borrowed, the, car, from, his, friend, ., &lt;end&gt;]</td>\n",
              "      <td>[&lt;start&gt;, il, a, emprunté, la, voiture, à, un, ami, ., &lt;end&gt;]</td>\n",
              "      <td>&lt;start&gt; he borrowed the car from his friend . &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; il a emprunté la voiture à un ami . &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49997</th>\n",
              "      <td>Who's coming for dinner?</td>\n",
              "      <td>Qui vient dîner ?</td>\n",
              "      <td>[&lt;start&gt;, who, 's, coming, for, dinner, ?, &lt;end&gt;]</td>\n",
              "      <td>[&lt;start&gt;, qui, vient, dîner, ?, &lt;end&gt;]</td>\n",
              "      <td>&lt;start&gt; who 's coming for dinner ? &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; qui vient dîner ? &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49999</th>\n",
              "      <td>I always thought Tom was funny.</td>\n",
              "      <td>J'ai toujours pensé que Tom était drôle.</td>\n",
              "      <td>[&lt;start&gt;, i, always, thought, tom, was, funny, ., &lt;end&gt;]</td>\n",
              "      <td>[&lt;start&gt;, j', ai, toujours, pensé, que, tom, était, drôle, ., &lt;end&gt;]</td>\n",
              "      <td>&lt;start&gt; i always thought tom was funny . &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; j' ai toujours pensé que tom était drôle . &lt;end&gt;</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>30815 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         en  ...                                                    fr_txt\n",
              "0                      What did I do wrong?  ...               <start> qu' ai - je fait de travers ? <end>\n",
              "2          I really appreciate your coming.  ...             <start> j' apprécie vraiment ta venue . <end>\n",
              "3                    I had to do something.  ...     <start> il fallait que je fasse quelque chose . <end>\n",
              "4                     I know Tom was first.  ...          <start> je sais que tom était le premier . <end>\n",
              "5                           They all drank.  ...                       <start> elles ont toutes bu . <end>\n",
              "...                                     ...  ...                                                       ...\n",
              "49993              That boy showed no fear.  ...           <start> ce garçon ne montra aucune peur . <end>\n",
              "49994       I couldn't stand it any longer.  ...     <start> je ne pourrais davantage le supporter . <end>\n",
              "49995  He borrowed the car from his friend.  ...         <start> il a emprunté la voiture à un ami . <end>\n",
              "49997              Who's coming for dinner?  ...                           <start> qui vient dîner ? <end>\n",
              "49999       I always thought Tom was funny.  ...  <start> j' ai toujours pensé que tom était drôle . <end>\n",
              "\n",
              "[30815 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fSDq61MLBLb6"
      },
      "source": [
        "#### 7. Convert the columns *en_txt* and *fr_txt* to lists of word indices using the fit_on_texts() and texts_to_sequences() functions of tensorflow.keras.preprocessing.text.Tokenizer (with parameter filters='', so we do not erase punctuation tokens). Pad these with zeros at the end of each sequence (using tensorflow.keras.preprocessing.sequence.pad_sequences) so that every sequence is of length *maxlen*, and save these as numpy arrays called *en_tensor* and *fr_tensor*. They should both be of shape (30684, 20).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-24T12:16:19.099770Z",
          "start_time": "2020-06-24T12:16:18.504672Z"
        },
        "colab_type": "code",
        "id": "qEiSbiFSM6yl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d3f8f578-408e-4c13-e655-705f3cb4c5c5"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer1 = Tokenizer(filters='')\n",
        "tokenizer1.fit_on_texts(sample_filt['en_txt'])\n",
        "tts_en = tokenizer1.texts_to_sequences(sample_filt['en_txt']) \n",
        "en_tensor = pad_sequences(tts_en, maxlen=20, padding='post')\n",
        "en_tensor.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30815, 20)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-24T18:28:28.623330Z",
          "start_time": "2020-06-24T18:28:27.962752Z"
        },
        "id": "HUoAOneUszMo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "84f80a83-9da0-48c7-d65b-7d61fb74657d"
      },
      "source": [
        "tokenizer2 = Tokenizer(filters='')\n",
        "tokenizer2.fit_on_texts(sample_filt['fr_txt'])\n",
        "tts_fr = tokenizer2.texts_to_sequences(sample_filt['fr_txt']) \n",
        "fr_tensor = pad_sequences(tts_fr, maxlen=20,padding='post')\n",
        "fr_tensor.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30815, 20)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5JaIUS7oBLYF"
      },
      "source": [
        "#### 8. Set variables en_nwords and fr_nwords to the number of possible values for elements in *en_tensor* and *fr_tensor* (i.e. the number of words in English and French, including the padding token). What result do you get for these numbers?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-24T18:26:51.213747Z",
          "start_time": "2020-06-24T18:26:51.167538Z"
        },
        "colab_type": "code",
        "id": "U1DK4uU7_kZJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "de1da0e3-97ec-4b80-be83-381eee427390"
      },
      "source": [
        "en_nwords = np.max(en_tensor)+1\n",
        "fr_nwords = np.max(fr_tensor)+1\n",
        "en_nwords, fr_nwords\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2187, 2497)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8ZTUOmsHM6zG"
      },
      "source": [
        "## Part 2: Building and running the seq2seq model\n",
        "\n",
        "**Now we will build a seq2seq model for automated translation, as described in lecture. The following imports will help you:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-24T18:32:38.029111Z",
          "start_time": "2020-06-24T18:32:38.015550Z"
        },
        "colab_type": "code",
        "id": "ugiWzQ_rM6zH",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Input, Embedding, GRU, Dense, TimeDistributed\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import numpy as np"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ErXUbyx8M6zK"
      },
      "source": [
        "**Use these hyperparameters for the model: (word embedding dimension, and hidden dimension of recurrent layers)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-24T18:32:38.848441Z",
          "start_time": "2020-06-24T18:32:38.835079Z"
        },
        "colab_type": "code",
        "id": "Nj0c_pvSOIoS",
        "colab": {}
      },
      "source": [
        "embedding_dim = 200\n",
        "hidden_dim = 1024"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7JwnIqgPM6zP"
      },
      "source": [
        "**We will now build our model. We have to use the Keras Functional API for this since the model is not simple enough to be written with the Sequential API.**\n",
        "\n",
        "### Questions:\n",
        "\n",
        "#### 9. Define a Keras model using the Functional API with the following layers:\n",
        "  * <b>Two input layers *en_inputs* and *fr_inputs*. Define their input shape to be (maxlen,).\n",
        "  * Two embedding layers: *en_embeddings* (applied to *en_inputs*) and *fr_embeddings* (applied to *fr_inputs*). Define the input and output dimensions using *en/fr_nwords* and *embedding_dim* as needed.\n",
        "  * A GRU recurrent layer with dimension *hidden_dim* applied to *en_embeddings*. Save its output in the variable *en_output*.\n",
        "  * A GRU recurrent layer with dimension *hidden_dim* applied to *fr_embeddings*. It should have return_sequences=True, and should start with initial_state equal to *en_output*. Save this layer's output in the variable *fr_gru_outputs*.\n",
        "  * Finally apply a dense layer, wrapped in TimeDistributed, to *fr_gru_outputs*. This dense layer should have softmax activation (so we get probabilities over words in French), and dimension *fr_nwords*. Save the output of this layer as *fr_outputs*.\n",
        "  * Define the model as *model = Model([en_inputs, fr_inputs], fr_outputs)*, check its architecture with *model.summary()*, and its input and output shapes with *model.input_shape* and *model.output_shape*.</b>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-24T18:32:40.030796Z",
          "start_time": "2020-06-24T18:32:40.017911Z"
        },
        "colab_type": "code",
        "id": "7OY6xLjfDWw9",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def get_model():\n",
        "    en_inputs = Input(shape=(maxlen, ))\n",
        "    fr_inputs = Input(shape=(maxlen, ))\n",
        "\n",
        "    en_embeddings = Embedding(input_dim = en_nwords, \n",
        "                              output_dim = embedding_dim)(en_inputs)\n",
        "    fr_embeddings = Embedding(input_dim = fr_nwords, \n",
        "                              output_dim = embedding_dim)(fr_inputs)\n",
        "    \n",
        "    en_output = GRU(hidden_dim)(en_embeddings)\n",
        "    \n",
        "    fr_gru_outputs = GRU(hidden_dim, \n",
        "                         return_sequences=True)(fr_embeddings, \n",
        "                                                initial_state = en_output)\n",
        "\n",
        "    fr_outputs = tf.keras.layers.TimeDistributed(Dense(fr_nwords, \n",
        "                                                       activation='softmax'))(fr_gru_outputs)\n",
        "                                                       \n",
        "    model = Model([en_inputs, fr_inputs], fr_outputs)\n",
        "    \n",
        "\n",
        "    return model"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-24T18:32:43.423750Z",
          "start_time": "2020-06-24T18:32:40.532342Z"
        },
        "id": "N01AENsAszM1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nmt = get_model()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-24T18:32:43.443591Z",
          "start_time": "2020-06-24T18:32:43.434174Z"
        },
        "id": "FdyNc27rszM3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d9d252b1-ef26-43bc-bc34-8075df959fed"
      },
      "source": [
        "nmt.input_shape, nmt.output_shape"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([(None, 20), (None, 20)], (None, 20, 2497))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-24T18:32:43.462180Z",
          "start_time": "2020-06-24T18:32:43.448468Z"
        },
        "id": "q-kvWu1UszM6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "outputId": "a7d15df4-5bcf-482e-d60d-fd556667983d"
      },
      "source": [
        "nmt.summary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 20)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 20)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 20, 200)      437400      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 20, 200)      499400      input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "gru (GRU)                       (None, 1024)         3766272     embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "gru_1 (GRU)                     (None, 20, 1024)     3766272     embedding_1[0][0]                \n",
            "                                                                 gru[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed (TimeDistribut (None, 20, 2497)     2559425     gru_1[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 11,028,769\n",
            "Trainable params: 11,028,769\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zpDpvvgHC-Vb"
      },
      "source": [
        "  \n",
        "#### 10. The model will predict the next token in French given the English sentence and the French tokens translated so far. To get the output labels, apply *np.roll* to *fr_tensor* so that the first column is the second token in French, the second column is the third token in French, and so forth. Apply *to_categorical* from Keras to this so that the output labels will be one-hot encoded, and save this in the variable *fr_tensor_2predict*. *fr_tensor_2predict* should be have shape (30684, 20, 2497).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-24T18:45:11.310100Z",
          "start_time": "2020-06-24T18:44:34.531516Z"
        },
        "colab_type": "code",
        "id": "wFltqil0DVgR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "24fa56a4-1cf2-46c9-948d-fcf062f1bb2c"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf.keras.backend.clear_session()  # For easy reset of notebook s\n",
        "\n",
        "Y = np.roll(fr_tensor,-1)\n",
        "fr_tensor_2predict = tf.keras.utils.to_categorical(Y)\n",
        "fr_tensor_2predict.shape"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30815, 20, 2497)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5ime0R-jC-ZR"
      },
      "source": [
        "\n",
        "#### 11. Compile the model with adam optimizer and categorical_crossentropy loss, and fit it on input *\\[en_tensor, fr_tensor\\]* and output *fr_tensor_2predict*. Recommended parameters are *batch_size = 64, epochs = 100, validation_split = 0.2*. Run training until validation loss stops decreasing by using early stopping ( *model.fit(..., callbacks = \\[EarlyStopping()\\])* ).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2020-06-24T18:54:07.766Z"
        },
        "colab_type": "code",
        "id": "ihZN6XiGDO8T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "66306fa9-9bfb-4954-df36-04bf57dca59d"
      },
      "source": [
        "tf.config.experimental_run_functions_eagerly(True)\n",
        "\n",
        "nmt.compile(optimizer='adam',\n",
        "            metrics=[\"accuracy\"],\n",
        "            loss='categorical_crossentropy')\n",
        "nmt.run_eagerly = True\n",
        "\n",
        "nmt.fit(x=[en_tensor, fr_tensor] , \n",
        "        y=fr_tensor_2predict,\n",
        "          batch_size = 64,\n",
        "          epochs=100,\n",
        "          validation_split = 0.2,\n",
        "          callbacks=tf.keras.callbacks.EarlyStopping(monitor='accuracy'))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "386/386 [==============================] - 28s 72ms/step - loss: 1.9081 - accuracy: 0.6746 - val_loss: 1.3689 - val_accuracy: 0.7554\n",
            "Epoch 2/100\n",
            "386/386 [==============================] - 27s 70ms/step - loss: 1.2210 - accuracy: 0.7633 - val_loss: 1.0909 - val_accuracy: 0.7884\n",
            "Epoch 3/100\n",
            "386/386 [==============================] - 27s 71ms/step - loss: 0.9600 - accuracy: 0.7992 - val_loss: 0.9300 - val_accuracy: 0.8093\n",
            "Epoch 4/100\n",
            "386/386 [==============================] - 27s 70ms/step - loss: 0.7959 - accuracy: 0.8199 - val_loss: 0.8167 - val_accuracy: 0.8274\n",
            "Epoch 5/100\n",
            "386/386 [==============================] - 27s 70ms/step - loss: 0.6275 - accuracy: 0.8484 - val_loss: 0.7349 - val_accuracy: 0.8394\n",
            "Epoch 6/100\n",
            "386/386 [==============================] - 27s 70ms/step - loss: 0.4894 - accuracy: 0.8731 - val_loss: 0.6764 - val_accuracy: 0.8515\n",
            "Epoch 7/100\n",
            "386/386 [==============================] - 27s 70ms/step - loss: 0.3661 - accuracy: 0.9000 - val_loss: 0.6409 - val_accuracy: 0.8616\n",
            "Epoch 8/100\n",
            "386/386 [==============================] - 27s 69ms/step - loss: 0.2715 - accuracy: 0.9233 - val_loss: 0.6258 - val_accuracy: 0.8674\n",
            "Epoch 9/100\n",
            "386/386 [==============================] - 27s 70ms/step - loss: 0.1973 - accuracy: 0.9437 - val_loss: 0.6250 - val_accuracy: 0.8702\n",
            "Epoch 10/100\n",
            "386/386 [==============================] - 27s 70ms/step - loss: 0.1518 - accuracy: 0.9562 - val_loss: 0.6354 - val_accuracy: 0.8713\n",
            "Epoch 11/100\n",
            "386/386 [==============================] - 27s 69ms/step - loss: 0.1138 - accuracy: 0.9677 - val_loss: 0.6434 - val_accuracy: 0.8724\n",
            "Epoch 12/100\n",
            "386/386 [==============================] - 27s 69ms/step - loss: 0.0938 - accuracy: 0.9740 - val_loss: 0.6546 - val_accuracy: 0.8733\n",
            "Epoch 13/100\n",
            "386/386 [==============================] - 27s 69ms/step - loss: 0.0792 - accuracy: 0.9787 - val_loss: 0.6823 - val_accuracy: 0.8691\n",
            "Epoch 14/100\n",
            "386/386 [==============================] - 27s 69ms/step - loss: 0.0712 - accuracy: 0.9813 - val_loss: 0.6781 - val_accuracy: 0.8745\n",
            "Epoch 15/100\n",
            "386/386 [==============================] - 27s 70ms/step - loss: 0.0670 - accuracy: 0.9827 - val_loss: 0.6951 - val_accuracy: 0.8746\n",
            "Epoch 16/100\n",
            "386/386 [==============================] - 27s 70ms/step - loss: 0.0668 - accuracy: 0.9827 - val_loss: 0.7044 - val_accuracy: 0.8729\n",
            "Epoch 17/100\n",
            "386/386 [==============================] - 27s 69ms/step - loss: 0.0637 - accuracy: 0.9838 - val_loss: 0.7178 - val_accuracy: 0.8714\n",
            "Epoch 18/100\n",
            "386/386 [==============================] - 27s 69ms/step - loss: 0.0632 - accuracy: 0.9841 - val_loss: 0.7921 - val_accuracy: 0.8447\n",
            "Epoch 19/100\n",
            "386/386 [==============================] - 27s 69ms/step - loss: 0.0627 - accuracy: 0.9840 - val_loss: 0.7257 - val_accuracy: 0.8741\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f47e0f13550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-tTGIXGKC-dm"
      },
      "source": [
        "\n",
        "#### 12. Write a function *run_translation(en_sentence)* that takes in a sentence *en_sentence* in English and translates it into French. You should first tokenize the sentence with spaCy, add &lt;start&gt; and &lt;end&gt; tokens and convert into padded vectors. Start with &lt;start&gt; as the first token in the translation, and then use the argmax of values output by model.predict() to predict the next token. Repeat this until &lt;end&gt; is output or until the output has length *maxlen*. <br>*Hint: This is very similar to the Trump tweet generation function.*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OO0cCe9NDPAt",
        "colab": {}
      },
      "source": [
        "\n",
        "def run_translation(sentence):\n",
        "  tokens = tokenize(en_model,sentence)\n",
        "          #en_tokenizer\n",
        "  tts_en = tokenizer1.texts_to_sequences([tokens])  \n",
        "  en_tensor = pad_sequences(tts_en, maxlen=20,  padding='post')\n",
        "\n",
        "  fr_tensor = np.zeros((1, 20))\n",
        "  fr_tensor[0][0] = 1   # 1:'<start>',...\n",
        "  target = []\n",
        "  i=0\n",
        "  while i < fr_tensor.shape[1]:\n",
        "    prediction = nmt.predict([en_tensor,fr_tensor])[0]\n",
        "    indx = np.argmax(prediction[i])\n",
        "    if indx == 2:  # if == '<end>'\n",
        "      break\n",
        "    fr_tensor[0][i+1] = indx\n",
        "    target.append(indx)\n",
        "    i+=1              #fr_tokenizer\n",
        "  print(sentence,'=>',tokenizer2.sequences_to_texts([target])[0],'\\n')\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "au7JmacGC-SX"
      },
      "source": [
        "\n",
        "#### 13. Uncomment the cell below and check the translations of the given sentences. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dfp-ohWnRaHn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "da3a7b25-fffb-4471-fc0b-5bc274c8c103"
      },
      "source": [
        "# uncomment this cell for question 13\n",
        "run_translation(\"This is not good!\")\n",
        "run_translation(\"This is scary.\")\n",
        "run_translation(\"I have a cat.\")\n",
        "run_translation(\"The dog is happy.\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is not good! => ça n' est pas bon   ! \n",
            "\n",
            "This is scary. => c' est utile . \n",
            "\n",
            "I have a cat. => j' ai une chatte . \n",
            "\n",
            "The dog is happy. => le chien est exact . \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "i4sjCV2xDL5E"
      },
      "source": [
        "\n",
        "### Bonus: \n",
        "#### Try translating some other sentences from English to French. Do you see any obvious problems with the results?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNBxj7ncj3Yu",
        "colab_type": "text"
      },
      "source": [
        "Yes, the result is not always correct"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3tbUxTlaDO4A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "dad7c61f-6768-4bf2-999f-dcabb006b26a"
      },
      "source": [
        "run_translation('we do not erase punctuation tokens')\n",
        "run_translation('Try translating some other sentences')\n",
        "\n",
        "run_translation('Uncomment the cell below and check the translations')\n",
        "\n",
        "run_translation('restart the runtime after running the cell below')\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "we do not erase punctuation tokens => nous ne avions pas le choix . \n",
            "\n",
            "Try translating some other sentences => essaie de l' aide . \n",
            "\n",
            "Uncomment the cell below and check the translations => le voleur s' est mis au cinéma . \n",
            "\n",
            "restart the runtime after running the cell below => le voleur s' est mis à aboyer . \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edRtCPjGhvz6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "outputId": "a69aae6a-13f7-4cae-cb4e-9fea15defc02"
      },
      "source": [
        "print(\"GOOGLE TRANSLATE:\\nwe do not erase punctuation tokens =>  nous n'effaçons pas les jetons de ponctuation\\n\")\n",
        "print(\"Try translating some other sentences =>  Essayez de traduire d'autres phrases\\n\")\n",
        "print(\"Uncomment the cell below and check the translations =>  Décommentez la cellule ci-dessous et vérifiez les traductions\\n\")\n",
        "print(\"restart the runtime after running the cell below => redémarrez le runtime après avoir exécuté la cellule ci-dessous\\n\")\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GOOGLE TRANSLATE:\n",
            "we do not erase punctuation tokens =>  nous n'effaçons pas les jetons de ponctuation\n",
            "\n",
            "Try translating some other sentences =>  Essayez de traduire d'autres phrases\n",
            "\n",
            "Uncomment the cell below and check the translations =>  Décommentez la cellule ci-dessous et vérifiez les traductions\n",
            "\n",
            "restart the runtime after running the cell below => redémarrez le runtime après avoir exécuté la cellule ci-dessous\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tp829cLvjlqf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}