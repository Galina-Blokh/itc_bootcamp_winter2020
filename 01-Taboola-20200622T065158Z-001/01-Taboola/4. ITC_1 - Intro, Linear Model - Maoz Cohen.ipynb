{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"4. ITC_1 - Intro, Linear Model.ipynb","provenance":[{"file_id":"13CKt0iOc0SmjMk9BOBhPp7EdHhESR2DD","timestamp":1580737115372},{"file_id":"1QSROBN8EKICeqIybDNwbvK8Jp0rx2SQi","timestamp":1580732099623}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"OhLUiz1Xo36V","colab_type":"code","colab":{}},"source":["# Israei Tech Challenge - Part 1 of 4\n","# Welcome to the ITC Taboola workshop. In this workshop we will address a real world problem:\n","# We have a list of users and items (ads) and their features. \n","# Lets try to predict the probablilty of a click (a user clicking on the commercial). \n","\n","# We are going to open the data, get to know it a little bit, then do a basic model in tensor flow. \n","\n","# Let's start with basic imports. \n","\n","import pandas as pd \n","import numpy as np\n","from collections import Counter, defaultdict\n","import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior() \n","import matplotlib.pyplot as plt\n","\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hi4eEemT-1WS","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9NkQq__0N4P2","colab_type":"code","colab":{}},"source":["from google.colab import files\n","uploaded = files.upload()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"W3EJkOTFo36Z","colab_type":"code","colab":{}},"source":["# Read the files\n","# These are real users and real advertisment from a single site.\n","# Start with 'ITC_20K.csv' (10MB), and later you can move to the ITC_40K.csv (20MB).\n","\n","data = pd.read_csv('ITC_20K.csv')\n","\n","# Basic clean-up \n","data.replace('', np.nan, inplace=True)\n","data = data.dropna()\n","\n","# Look at the data columns. Do you understand what they mean?  \n","data.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LubdI90Ko36c","colab_type":"code","colab":{}},"source":["# Try to understand the meaning of each column\n","# Separate columns to user features and source/context/publisher features\n","\n","source_features = \"source_id,content_category,ad_type,quality_level,source_item_type,syndicator_id,\\\n","                    target_id,campaign_id,title,campaign_language\".split(\",\")\n","user_features = \"user_id,browser_platform,os_family,country_code,os_name,country,region,browser_name,\\\n","                 user_clicks,user_recs,prev_syndicator_clicks,target_recs,campaign_recs,\\\n","                 user_category_clicks,user_category_recs\".split(\",\")\n","\n","label = \"is_click\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yahoFUmPo36e","colab_type":"code","colab":{}},"source":["# Our machine learning algorithm will handle each column according to his type. \n","# Task: Separate columns to lists of numeric and categorical features. notice that even columns that are numbers\n","# may be categorical features, like _id features. \n","# Notice we have numerical features labeled as objects since they are arrays! Let's leave them out for now. \n","\n","numeric_features = ## ADD YOUR CODE HERE\n","categorical_features = ## ADD YOUR CODE HERE"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8e0iJd4zo36h","colab_type":"code","colab":{}},"source":["# A basic check with data.dtypes shows that the python read_csv may read the format wrongfully\n","print(data.dtypes)\n","\n","# To make sure all the numeric data is formated, lets do a basic clean-up of numerical data\n","data[numeric_features] = data[numeric_features].apply(pd.to_numeric, errors='coerce')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NCHAgqQvo36j","colab_type":"code","colab":{}},"source":["# Let's answer a few basic questions before we start: \n","\n","# Task: How big is our data set? \n","data_size = ## ADD YOUR CODE HERE\n","total_num_features = ## ADD YOUR CODE HERE\n","\n","print(f'Number of samples: {data_size}')\n","print(f'Number of features: {total_num_features}')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3vfiHolfo36l","colab_type":"code","colab":{}},"source":["# Task: How many people clicked / haven't clicked? \n","\n","num_clickers = ## ADD YOUR CODE HERE\n","num_no_click = ## ADD YOUR CODE HERE\n","\n","print(f'Number of clickers: {num_clickers}')\n","print(f'Number of non clickers: {num_no_click}')\n","print(f'Clickers-nonclickers ratio: {num_clickers/(num_no_click+num_clickers)}')\n","\n","# num_clicks = data['is_click'].value_counts()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"E6k_nziTo36n","colab_type":"code","colab":{}},"source":["# From what browser people are coming for this publisher? \n","# Task: Plot or count. Is this data usual? \n","\n","## ADD YOUR CODE HERE"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BKB8PsDlo36q","colab_type":"code","colab":{}},"source":["# Lets try to catch some interesting \"signal\". \n","# It is always helpful to to plot the data before starting. \n","# Try to find some connection between our numerical features and the \"is_click\" columns. \n","# If you're going to use scatter, it is sometimes easier to add noise to the is_click column,\n","# to help see the spread of the data. \n","# This MIGHT help: \n","# noise = np.random.randn(data_size)/10\n","# is_clicked = data[\"is_click\"].values + noise\n","\n","# HINT - import seaborn as sns\n","\n","## ADD YOUR CODE HERE"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iA8Phxa4o36v","colab_type":"code","colab":{}},"source":["# Enough exploration! \n","# Let's try to build our first model: A simple logistic regression, only using our numerical features. \n","\n","# Only numercial features\n","only_num_data = data[numeric_features].dropna()\n","list(only_num_data)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_33WDzkqo360","colab_type":"code","colab":{}},"source":["# Task: Create msk to split into train, test sets\n","train_pctg = 0.8\n","train_size = ## ADD YOUR CODE HERE\n","msk = ## ADD YOUR CODE HERE"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BHokjOk6o369","colab_type":"code","colab":{}},"source":["# train, test split\n","train = only_num_data[msk]\n","test = only_num_data[~msk]\n","\n","# Separate the label columns from our features\n","y_train = train[\"is_click\"]\n","del train[\"is_click\"]\n","y_test = test[\"is_click\"]\n","del test[\"is_click\"]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6q9hizSco36_","colab_type":"code","colab":{}},"source":["# Let's start with logistic regression with TensorFlow. \n","# We are going to use the 5 numeric features we've gotten to know.\n","# This code is roughly based on: \n","# https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/2_BasicModels/logistic_regression.ipynb\n","\n","# Important parameters for the model\n","# You can play with these, as you wish. \n","\n","num_samples = train.shape[0]\n","num_features = train.shape[1] \n","batch_size = 64\n","training_epochs = 10\n","total_batch = int(num_samples/batch_size)\n","learning_rate = 0.08\n","print_every = 1 # epochs"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vh_QVDUzo37D","colab_type":"code","colab":{}},"source":["# Lets build our TF Graph step-by-step"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aE5MbMhdo37F","colab_type":"code","colab":{}},"source":["# TF Graph Input\n","\n","# Dataset of features\n","x = tf.placeholder(tf.float32, [None, num_features])\n","\n","# Labels\n","y = tf.placeholder(tf.float32, [None, 1])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vYzTfHIAo37I","colab_type":"code","colab":{}},"source":["# Set model variable (= what the model is going to learn)\n","\n","W = tf.Variable(tf.random_normal([num_features, 1], stddev=0.15), name=\"weights\")\n","b = tf.Variable(tf.zeros([1]), name=\"bias\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zihjl5Lfo37M","colab_type":"code","colab":{}},"source":["# Create the model operations. \n","# These are different calculations of the placeholder and variables. \n","\n","# Initialize the variables \n","init = tf.global_variables_initializer()\n","\n","# Construct model\n","pred = tf.sigmoid(tf.matmul(x, W) + b)\n","\n","# Minimize error using MSE\n","loss = tf.losses.mean_squared_error(labels = y, predictions = pred)\n","\n","# Gradient Descent\n","optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xo8CpoBno37n","colab_type":"code","colab":{}},"source":["# Start TensorFlow learning session. \n","\n","# Start training\n","def train_session(train, test, y_train, y_test):\n","    with tf.Session() as sess:\n","\n","        # Run the initiation operation (initializer)\n","        sess.run(init)\n","\n","        # Training cycle\n","        for epoch in range(training_epochs):\n","            avg_loss = 0.\n","\n","            # Loop over all batches\n","            for i in range(total_batch):\n","\n","                # Build inputs in every batch loop\n","                batch_xs = train.iloc[i*batch_size : (i+1)*batch_size].values   \n","                batch_ys = y_train.iloc[i*batch_size : (i+1)*batch_size]\n","                batch_ys = batch_ys.values.reshape([batch_size,1])\n","\n","                # Run optimization operation (backprop) and loss operation (to get loss value)\n","                _, c = sess.run([optimizer, loss], feed_dict={x: batch_xs, y: batch_ys})\n","\n","                # Compute average loss\n","                avg_loss += c / (1.0*total_batch)\n","\n","            # Display logs per epoch step\n","            if (epoch+1) % print_every == 0:\n","                 print(\"Epoch:\", '%02d' % (epoch+1), \"loss=\", avg_loss)\n","\n","        print(\"Optimization Finished!\")\n","\n","        # Test model\n","        correct_prediction = tf.equal(tf.round(pred), y)\n","        # Calculate accuracy\n","        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n","        batch_xs = test.values            \n","        batch_ys = y_test\n","        batch_ys = batch_ys.values.reshape([len(y_test),1])\n","        print(\"Accuracy:\", accuracy.eval({x: batch_xs, y: batch_ys}))     "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VUJlQqSho370","colab_type":"code","colab":{}},"source":["train_session(train, test, y_train, y_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lzY1vkWCo374","colab_type":"code","colab":{}},"source":["# The results should be about 48% \n","# This is VERY low. \n","# What are we missing ?\n","# Try again with using normalization on the numeric columns. \n","# Why should this help ? "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qRazOSsro378","colab_type":"code","colab":{}},"source":["def normalize(df):\n","    result = df.copy()\n","    for feature_name in df.columns:\n","        max_value = df[feature_name].max()\n","        min_value = df[feature_name].min()\n","        if (max_value - min_value) > 0:\n","            result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n","    return result"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FypPnv6zN7He","colab_type":"code","colab":{}},"source":["only_num_data.drop('is_click', axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FoxNBr57o37_","colab_type":"code","colab":{}},"source":["# train, test split\n","normalized_data = normalize(only_num_data.drop('is_click', axis=1))\n","train = normalized_data[msk]\n","test = normalized_data[~msk]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QyWPngF7o38D","colab_type":"code","colab":{}},"source":["# Re-run the training\n","# Exactly the same training from before! We are using the exact same graph with a different input.\n","train_session(train, test, y_train, y_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YYB3GYlBo38F","colab_type":"code","colab":{}},"source":["# Almost 55% - definetly a bit better!\n","# We're going to add 2 columns of numerical features to our data and do the calculation once again:\n","# These features are past_user_category_recs and past_user_category_clicks\n","# We have to parse these with there two ready functions: \n","\n","# A function that takes in a single cell that has array value and \n","# makes sure all arrays are the same length = max_values\n","def split_cell_to_list(x, max_values=24, pad_value=[0], value_type='float'):\n","    if type(x)==str: \n","        x = x.split(' ')\n","        if len(x) >= max_values:\n","            x = x[:max_values]\n","        else: \n","            x = x + pad_value*(max_values-len(x))\n","        return x\n","    else: \n","        print('cell with bad content')\n","        print(type(x),x)\n","\n","# A function that receives a dataframe and a column_name and explodes \n","# that column to num_columns different columns          \n","def parse_and_add_columns(df, column_name):\n","    if column_name in list(df):\n","        df[column_name] = df[column_name].apply(split_cell_to_list, args=())   \n","        temp = pd.DataFrame(df[column_name].values.tolist())\n","        temp = temp.rename(columns=lambda x: column_name+str(x))\n","        df = pd.concat([df, temp], axis=1, join='inner')\n","        del df[column_name]\n","    else: \n","        print('columns already parsed!')\n","    return df"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VPX_Z9WXo38H","colab_type":"code","colab":{}},"source":["# two more columns\n","numeric_features = \"user_recs,prev_syndicator_clicks,target_recs,campaign_recs,user_clicks,\\\n","user_category_clicks,user_category_recs,is_click\".split(\",\") \n","only_num_data = data[numeric_features]\n","\n","# print the data set before parsing\n","only_num_data.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nwJfXRkwo38L","colab_type":"code","colab":{}},"source":["# parse data with given function \"parse_and_add_columns\"\n","\n","only_num_data = parse_and_add_columns(only_num_data, 'user_category_recs')\n","only_num_data = parse_and_add_columns(only_num_data, 'user_category_clicks')\n","\n","only_num_data.head()\n","\n","# How many new features did we get? "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"orgn6vAto38O","colab_type":"code","colab":{}},"source":["# Split into train, test sets\n","normalized_data = normalize(only_num_data.drop('is_click', axis=1).apply(pd.to_numeric, errors='coerce'))\n","train = normalized_data[msk]\n","test = normalized_data[~msk]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wyWcCq9no38Q","colab_type":"code","colab":{}},"source":["# we need to restart our graph, because now we have a different num_features -> \n","# Some of our variables are going to have different sizes\n","\n","tf.reset_default_graph()\n","\n","# Exactly the code from before! BUT we need to run it again since num_features is different now. \n","\n","num_features = len(list(train))\n","num_samples = train.shape[0]\n","batch_size = 50\n","training_epochs=13\n","total_batch = int(num_samples/batch_size)\n","learning_rate = 0.8\n","\n","# tf Graph Input\n","x = tf.placeholder(tf.float32, [None, num_features])\n","y = tf.placeholder(tf.float32, [None, 1])\n","\n","# Set model weights\n","# W = tf.Variable(tf.zeros([num_features, 1]), name=\"weight\")\n","W = tf.Variable(tf.random_normal([num_features, 1], stddev=0.05), name=\"weights\")\n","b = tf.Variable(tf.zeros([1]), name=\"bias\")\n","\n","# Construct model\n","matmul = tf.matmul(x, W)\n","pred = tf.sigmoid(matmul + b)\n","\n","# Minimize error using cross entropy\n","loss = tf.losses.mean_squared_error(labels = y, predictions = pred)\n","\n","# Gradient Descent\n","optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n","display_step = 1\n","\n","# Initialize the variables (i.e. assign their default value)\n","init = tf.global_variables_initializer()\n","\n","# Re-run the training\n","train_session(train, test, y_train, y_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eah4tAkMo38U","colab_type":"code","colab":{}},"source":["# If you have time: \n","# Change the learning rate.\n","# Change the loss function.\n","# What's the best accuracy you've got? "],"execution_count":0,"outputs":[]}]}