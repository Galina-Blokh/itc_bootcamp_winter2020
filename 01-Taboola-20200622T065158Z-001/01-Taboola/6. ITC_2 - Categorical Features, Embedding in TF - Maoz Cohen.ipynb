{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"6. ITC_2 - Categorical Features, Embedding in TF.ipynb","provenance":[{"file_id":"1TqrGh_zia64S3TQ5QTP5Gxxp1JDFmA69","timestamp":1581346311940},{"file_id":"15VgnT4Maw78XafnelvNRc90Q2lSefBHY","timestamp":1581336763372},{"file_id":"1s0F9Qhet2CEuWtG56-3GOwT3f6nl34pm","timestamp":1548844220985}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"C-5KyZibqWSe","colab_type":"code","colab":{}},"source":["from google.colab import files\n","uploaded = files.upload()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uZZWBZG01sax","colab_type":"code","colab":{}},"source":["%tensorflow_version 1.x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gURTM6jPqSZY","colab_type":"code","colab":{}},"source":["# Israei Tech Challenge - Part 2 of 4\n","# Welcome to the ITC Taboola workshop. In this workshop we will address a real world problem:\n","# have a list of users and items (commercials) and their features. \n","# Lets try to predict the probablilty of a click (a user clicking on the commercial). \n","\n","# After only using only numerical features, it's time to use our categorical features!\n","# To Do that we'll need to use embedding in our tensor flow graph. \n","\n","# Let's start with basic imports. \n","import pandas as pd \n","import numpy as np\n","from collections import Counter, defaultdict\n","import ast\n","import os\n","import operator\n","import csv\n","from numbers import Number\n","from tensorflow.contrib.tensorboard.plugins import projector\n","import tensorflow as tf\n","from tensorboardcolab import TensorBoardColab, TensorBoardColabCallback\n","import matplotlib.pyplot as pltb \n","%matplotlib inline\n","low_memory=False\n","\n","# Read Data\n","data = pd.read_csv('ITC_20K.csv')\n","\n","# Basic clean-up\n","data.replace('', np.nan, inplace=True)\n","data = data.dropna()\n","data.head(2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vwoPI0cvqSZc","colab_type":"code","colab":{}},"source":["# These are the features which have a categorical value for their columns. \n","\n","features_for_embedding = [\n","    'source_id',\n","    'content_category',\n","    'ad_type',\n","    'quality_level',\n","    'source_item_type', \n","    'syndicator_id',\n","    'target_id',\n","    'campaign_id',\n","    'campaign_language',\n","    'user_id',\n","    'browser_platform',\n","    'os_family',\n","    'country_code',\n","    'os_name',\n","    'country',\n","    'region',\n","    'browser_name'\n","]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bD3i-R9KqSZf","colab_type":"text"},"source":["# (ANALYSIS) LOOKING AT THE DATA\n","How many unique values there're for each category?\n","How does the distribution looks like? (how many occurances for each value within each category?)"]},{"cell_type":"markdown","metadata":{"id":"WRWVMji-qSZf","colab_type":"text"},"source":["# (CODING) PREPROCESSING:\n","For each category column we wish to create *another* column for representing the possible values as integers (like enum)\n","1. The new column name should be same as original with the suffix \"_mapped\"\n","2. Start your indexing from 1 - keeping \"0\" for OOV (Out Of Volcabulary)\n","3. Pay attention to deside on an OOV threshold (num occurances).\n","4. Your output should be both:\n","\n","    4.1 New columns in the original dataframe\n","\n","    4.2 A dictionary of dictionaries named \"look_up_dict\": which maps from category_feature to a look_up dictionary (key: category_value, value: index)\n","    \n","data[\"sourced_id_mapped\"] = ...\n","\n","look_up_dict = {\"source_id\": {\"I am a possible value\": 1, \"OOV\": 0, ...}, \"content_category\": {\"OOV\":0, \"good\": 3,...},...}"]},{"cell_type":"markdown","metadata":{"id":"PTgLbagiqSZg","colab_type":"text"},"source":["# Your solution:"]},{"cell_type":"code","metadata":{"id":"GDR6APsAC6w1","colab_type":"code","colab":{}},"source":["# DATA ANALYSIS:\n","\n","# YOUR CODE HERE"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pOP-CKW8ErkW","colab_type":"code","colab":{}},"source":["# DATA PROCESSING\n","\n","# YOUR CODE HERE"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F-JWUWd3qSZi","colab_type":"text"},"source":["# The code we give you:"]},{"cell_type":"code","metadata":{"id":"9ubD850tqSZj","colab_type":"code","colab":{}},"source":["# Split into train, test sets\n","msk = np.random.rand(len(data)) < 0.8 \n","train = data[msk]\n","test = data[~msk]\n","\n","train = train.dropna()\n","test = test.dropna()\n","\n","# Create label\n","y_train = train[\"is_click\"]\n","del train[\"is_click\"]\n","y_test = test[\"is_click\"]\n","del test[\"is_click\"]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kzg9nWmOqSZl","colab_type":"code","colab":{}},"source":["EMBEDDINGS_COLLECTION_NAME = \"embeddings\"\n","# a function for adding a categorical placeholder\n","def add_categorical_placeholder(input_name):\n","    batch_size = None  # using dynamic batch\n","    placeholder = tf.placeholder(tf.int32, shape=batch_size, name=input_name)\n","    tf.add_to_collection(\"input\", placeholder)\n","    return placeholder\n","\n","# a function that takes a placeholder and creates embedding for it\n","def add_categorical_embedding(input_placeholder, input_dim, output_dim):\n","    input_name = input_placeholder.op.name\n","    uniform = tf.random_uniform(shape=[input_dim, output_dim],minval=0.05,maxval=0.05)\n","    emb_weights = tf.Variable(initial_value=uniform, name=('{}_weights'.format(input_name)))\n","    tf.add_to_collection(EMBEDDINGS_COLLECTION_NAME, emb_weights)\n","    emb = tf.nn.embedding_lookup(emb_weights, input_placeholder, name=('{}_lookup'.format(input_name)))\n","    emb.set_shape([None, output_dim])\n","    return emb"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FOmMltp-qSZn","colab_type":"code","colab":{}},"source":["embedding_size = 4\n","\n","def create_embeddings_for_all_features(features_for_embedding, look_up_dict):\n","    embeddings = []\n","    placeholder_list = []\n","\n","    feed_dict = {}\n","    num_features = 0\n","    for feature in features_for_embedding:\n","        input_placeholder = add_categorical_placeholder(feature+'_mapped')\n","        placeholder_list.append(input_placeholder)\n","        emb = add_categorical_embedding(input_placeholder, len(look_up_dict[feature]), embedding_size)\n","        \n","        embeddings.append(emb)\n","        num_features += embedding_size \n","    return num_features, placeholder_list, embeddings"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BIapgenTqSZp","colab_type":"code","colab":{}},"source":["def _safe_encode(value_0):\n","    if isinstance(value_0, Number):\n","        value_0 = str(value_0)\n","    try:\n","        value = value_0.encode(\"utf8\")\n","    except:\n","        value = value_0\n","    return value\n","  \n","def create_embedding_metadata(embedding_name, embedding_lookup, model_log_dir):\n","    sorted_emb = sorted(embedding_lookup.items(), key=operator.itemgetter(1))\n","    embedding_filename = embedding_name + '.tsv'\n","    metadata_path = os.path.join(model_log_dir, embedding_filename)\n","    with open(metadata_path, 'w') as f:\n","        metadata_writer = csv.writer(f, delimiter='\\t')\n","        for value in sorted_emb:\n","            value = _safe_encode(value[0])\n","            if embedding_name in ['target_id']:\n","                metadata_writer.writerow(['#'+str(value)])  # add some char so tensorboard will show as string\n","            else:\n","                metadata_writer.writerow([str(value)])\n","\n","    return model_log_dir + '/' + embedding_filename\n","\n","def project_embeddings(tbc):\n","    embedding_projector = projector.ProjectorConfig()\n","    metadata_paths = {}\n","    \n","    for i in range(len(features_for_embedding)):\n","        embedding_name = features_for_embedding[i]\n","        metadata_paths[embedding_name] = create_embedding_metadata(embedding_name, look_up_dict[embedding_name], model_log_dir)\n","        \n","    embeddings_vars = tf.get_collection_ref(EMBEDDINGS_COLLECTION_NAME)\n","    for embedding_var in embeddings_vars:\n","        embedding = embedding_projector.embeddings.add()\n","        embedding_name = embedding_var.name[:-len(\"_mapped_weights:0\")]\n","        embedding.tensor_name = embedding_var.name\n","        embedding.metadata_path = metadata_paths[embedding_name]\n","\n","    projector.visualize_embeddings(tbc.get_writer(), embedding_projector)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"x1Zadil5lKte","colab_type":"code","colab":{}},"source":["data"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5nLIvam5qSZu","colab_type":"text"},"source":["# Building the graph and training"]},{"cell_type":"code","metadata":{"id":"eUKdbAb0qSZv","colab_type":"code","colab":{}},"source":["! rm -rf /tmp/taboola_tutorial/logs/*"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GbKz6CTQqSZ0","colab_type":"code","colab":{}},"source":["# This is the same code as before! except we move the check on out test dataframe inside the loop,\n","# so we can track the improvement of our model \n","\n","\n","# Parameters\n","num_samples = train.shape[0]\n","batch_size = 50\n","training_epochs=5\n","total_batch = int(num_samples/batch_size)\n","learning_rate = 0.25\n","\n","# TF model\n","# This is where we start creating our TF graph\n","tf.reset_default_graph()\n","# tf Graph Input\n","num_features, placeholder_list, embeddings = create_embeddings_for_all_features(features_for_embedding, look_up_dict)\n","    \n","y = tf.placeholder(tf.float32, [None, 1])\n","\n","# Set model weights\n","x = tf.concat(axis=1, values=embeddings, name='concat')\n","\n","# if more layers \n","# layer_size = 20\n","# W1 = tf.Variable(tf.random_normal([num_features, 20], stddev=0.15), name=\"weights\")\n","# b1 = tf.Variable(tf.zeros([20]), name=\"bias\")\n","#  + relu \n","W = tf.Variable(tf.random_normal([num_features, 1], stddev=0.15), name=\"weights\")\n","b = tf.Variable(tf.zeros([1]), name=\"bias\")\n","\n","pred = tf.sigmoid(tf.matmul(x, W) + b)\n","\n","# Minimize error using MSE\n","# cost = tf.reduce_sum(tf.square(y-pred))\n","cost = tf.losses.mean_squared_error(labels = y, predictions = pred)\n","\n","# Gradient Descent\n","global_step = tf.Variable(0,name='global_step', trainable=False)\n","optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost, global_step=global_step)\n","\n","# Test model\n","correct_prediction = tf.equal(tf.round(pred), y)\n","# Calculate accuracy\n","accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n","\n","# Define test_feed_dict once:\n","test_feed_dict = {}\n","for placeholder in placeholder_list:\n","    feature = placeholder.name[:-2]\n","    test_feed_dict[placeholder] = test[feature].values\n","test_feed_dict[y] = y_test\n","test_feed_dict[y] = test_feed_dict[y].values.reshape([len(y_test),1])\n","\n","# Initialize the variables (i.e. assign their default value)\n","init = tf.global_variables_initializer()\n","saver = tf.train.Saver()\n","model_log_dir = \"/tmp/taboola_tutorial/logs\"\n","summary_writer = tf.compat.v1.summary.FileWriter(model_log_dir)\n","\n","# Start training\n","sess = tf.Session()\n","with sess:\n","    tbc = TensorBoardColab(graph_path=model_log_dir)\n","    loss_summary = tf.summary.scalar('per_batch_loss', cost)\n","    merge_summaries_op = tf.summary.merge_all()\n","    # Run the initializer\n","    sess.run(init)\n","\n","    # Training cycle\n","    for epoch in range(training_epochs):\n","        avg_cost = 0.\n","\n","        # Loop over all batches\n","        for i in range(total_batch):\n","            feed_dict = {}\n","         \n","            for placeholder in placeholder_list:\n","                feature = placeholder.name[:-2]\n","                feed_dict[placeholder] = train[feature].iloc[i*batch_size : (i+1)*batch_size].values\n","\n","            feed_dict[y] = y_train.iloc[i*batch_size : (i+1)*batch_size]\n","            feed_dict[y] = feed_dict[y].values.reshape([batch_size,1])\n","\n","            # Run optimization op (backprop) and cost op (to get loss value)\n","            _, c, merged_summary, current_global_step = sess.run([optimizer, cost, merge_summaries_op, global_step], feed_dict)\n","            summary_writer.add_summary(merged_summary,current_global_step)\n","\n","            # Compute average loss\n","            avg_cost += c / (1.0*total_batch)\n","            \n","        print(current_global_step)\n","        tbc.save_value(\"loss\", \"train\", epoch, avg_cost)\n","\n","        # Calc test error:\n","        test_loss = sess.run(cost, test_feed_dict)\n","        tbc.save_value(\"loss\", \"test\", epoch, test_loss)\n","        \n","        # Display logs per epoch step\n","        print(\"Epoch:\", '%02d' % (epoch+1))\n","        print(\"train error:\\t\", avg_cost)\n","        print(\"test error:\\t\", test_loss)\n","        print(\"Accuracy:\", accuracy.eval(test_feed_dict))\n","    \n","        saver.save(sess, os.path.join(model_log_dir, \"model.ckpt\"), current_global_step)\n","        tbc.flush_line('loss') \n","tbc.save_value(\"loss\", \"train\", 10, 0.1)\n","project_embeddings(tbc)\n","tbc.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QZwVwMj3x-O7","colab_type":"code","colab":{}},"source":["! ls -l /tmp/taboola_tutorial/logs | grep tsv"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bb2OSJ4_MfUH","colab_type":"code","colab":{}},"source":["files.download('/tmp/taboola_tutorial/logs/quality_level.tsv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gr-U_tHDJ78_","colab_type":"code","colab":{}},"source":["files.download('/tmp/taboola_tutorial/logs/os_name.tsv') "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ecJw4ncrr2eG","colab_type":"code","colab":{}},"source":["files.download('/tmp/taboola_tutorial/logs/browser_name.tsv') "],"execution_count":0,"outputs":[]}]}