{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V0W3vlijxKLd"
   },
   "source": [
    "# HMM Exercise\n",
    "\n",
    "#### In this exercise, we will use the ``hmmlearn`` library to practice what we have learned about Hidden Markov Models (HMMs), both on a toy problem and to build a POS tagger from scratch. First make sure that ``hmmlearn`` is installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5791,
     "status": "ok",
     "timestamp": 1589740623144,
     "user": {
      "displayName": "Morris Alper",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjhQNy-ts-7hx9PAJ0YHgAIjwD0nfjAtMRdfDrCyQ=s64",
      "userId": "15842932163458061285"
     },
     "user_tz": -180
    },
    "id": "u_sxxN-m7LK2",
    "outputId": "6ff7f184-90f1-4c6d-e844-e962d95939d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hmmlearn in /home/gal/anaconda3/lib/python3.6/site-packages (0.2.3)\n",
      "Requirement already satisfied: scikit-learn>=0.16 in /home/gal/anaconda3/lib/python3.6/site-packages (from hmmlearn) (0.23.1)\n",
      "Requirement already satisfied: scipy>=0.15 in /home/gal/anaconda3/lib/python3.6/site-packages (from hmmlearn) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.10 in /home/gal/anaconda3/lib/python3.6/site-packages (from hmmlearn) (1.18.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/gal/anaconda3/lib/python3.6/site-packages (from scikit-learn>=0.16->hmmlearn) (2.0.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/gal/anaconda3/lib/python3.6/site-packages (from scikit-learn>=0.16->hmmlearn) (0.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install hmmlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/gal/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/gal/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from hmmlearn.hmm import MultinomialHMM\n",
    "from sklearn.metrics import  accuracy_score\n",
    "\n",
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "warnings.filterwarnings('ignore', category=ImportWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ThDA-4eSH-Tr"
   },
   "source": [
    "# Part 1: Seasonal weather\n",
    "\n",
    "**In this section, we will consider the following toy problem:**\n",
    "\n",
    "**A year has four seasons - winter, spring, summer, and fall. Every day of the year we go outside, look at the sky to see if it is sunny or cloudy, and try to guess what season it is.**\n",
    "\n",
    "## Questions:*\n",
    "### 1. If we model this with an HMM, what are the hidden states? What are the observed variables?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = {'winter':0, 'spring':1, 'summer':2, 'fall':3} #n_components\n",
    " \n",
    "observed_variables = {'rainy':0, 'sunny':1} #n_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Using ``hmmlearn.hmm.MultinomialHMM``, create an HMM model for this problem. Note: you may refer to [the hmmlearn API documentation](https://hmmlearn.readthedocs.io/en/latest/api.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  * Instantiate the HMM model with ``weather_model = MultinomialHMM(n_components=...)``, where ``n_components`` is set to the number of hidden states.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_model = MultinomialHMM(n_components=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  * Set ``weather_model.startprob_`` so that the model starts in winter (probability 1 for winter and 0 for other seasons). Hint: ``weather_model.startprob_`` should be a list of numbers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_model.startprob_ = [1, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  * Set ``weather_model.transmat_`` so that on every day, there is a 0.99 probability of the season remaining the same as the previous day and a 0.01 probability of it being the next season. Hint: ``model.transmat_`` should be a NumPy array of shape ``(n_components, n_components)``\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_model.transmat_ = np.array([[0.99, 0.01, 0, 0],\n",
    "                                    [0, 0.99, 0.01, 0],\n",
    "                                    [0, 0, 0.99, 0.01],\n",
    "                                    [0.01, 0, 0, 0.99]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  * Set ``weather_model.emissionprob_`` so that 90% of summer days are sunny, 90% of winter days are rainy, and 50% of spring and fall days are sunny or rainy. Hint: ``model.emissionprob_`` should be a NumPy array of shape ``(n_components, n_features)``.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_model.emissionprob_ = np.array([[0.1, 0.9],\n",
    "                                        [0.5, 0.5],\n",
    "                                        [0.9, 0.1],\n",
    "                                        [0.5, 0.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Sample 100 days from the model by using  ``weather_model.sample(100)``.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_matrix, state_sequence = weather_model.sample(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### How many sunny days are observed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample_matrix[sample_matrix == 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### How many times did the season change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "#first way - simple\n",
    "count = 0\n",
    "for i in range(1,len(state_sequence)):\n",
    "    if state_sequence[i-1] != state_sequence[i]:\n",
    "        count+=1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#second way from the box\n",
    "from more_itertools import unique_justseen\n",
    "\"\"\"List unique elements, preserving order. Remember only the element just seen.\"\n",
    "    # unique_justseen('AAAABBBCCDAABBB') --> A B C D A B\n",
    "    # unique_justseen('ABBCcAD', str.lower) --> A B C A D\"\"\"\n",
    "\n",
    "len(list(unique_justseen(state_sequence)))-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 1 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "state_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the changes from 0 to 1 and from 1 to 2, i.e. It was a season change from winter to spring and from spring to summer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. If 50 sunny days and then 50 rainy days are observed, what is the most likely sequence of seasons for those days under this model? Use ``weather_model.decode(...)`` to determine this. Hint: The input to this function should be an array of shape`` (100, 1)`` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_days = np.concatenate((np.zeros(50, dtype=int), np.ones(50, dtype=int))).reshape(-1,1)\n",
    "arr_days.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-33.2843121229581,\n",
       " array([0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logprob, seq = weather_model.decode(arr_days)\n",
    "logprob, seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see, that the majority is 0 and 2, i.e. winter and summer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 51, 1: 1, 2: 47, 3: 1}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_var, val_count = np.unique(seq, return_counts = True)\n",
    "\n",
    "dict(zip(uni_var,val_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ddTB2_igH0vg"
   },
   "source": [
    "# Part 2: Building a POS tagger\n",
    "\n",
    "I**n this section, we will build a simple Part-of-Speech (POS) tagger for English texts based on labelled data from the [Brown Corpus](https://en.wikipedia.org/wiki/Brown_Corpus). First make sure that the necessary corpora are downloaded:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MtsiB6X2p7-z"
   },
   "source": [
    "**Now we will build a HMM-based POS tagger from scratch, step-by-step:**\n",
    "## Questions:\n",
    "\n",
    "### 5. Get POS-tagged sentences from the Brown corpus using \n",
    "\n",
    "``nltk.corpus.brown.tagged_sents(tagset='universal')``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = nltk.corpus.brown.tagged_sents(tagset='universal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use ``sklearn.model_selection.train_test_split`` to split them into 80% training data and 20% testing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45872, 11468)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8, 0.2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)/len(data), len(test)/len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Define ``pos_tags`` to be an array of all unique POS tags found in the training data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.',\n",
       " 'ADJ',\n",
       " 'ADP',\n",
       " 'ADV',\n",
       " 'CONJ',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'NUM',\n",
       " 'PRON',\n",
       " 'PRT',\n",
       " 'VERB',\n",
       " 'X'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tags = (set([tag for item in train for word, tag in item]))\n",
    "pos_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many unique POS tags were found?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Using collections.Counter, find the 5000 most common word tokens in the training data, and save their unique values to an array ``vocab``. Make all words lowercase before counting, and in addition, add the token '[UNK]' as the first element of ``vocab`` (representing words which are \"out of vocabulary\"). Hint: the first five elements of ``vocab`` should be \\[\"\\[UNK\\]\", \"the\", \",\", \".\", \"of\", ...\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]', 'the', ',', '.', 'of']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = Counter([word.lower() for item in train for word, tag in item]).most_common(5000)\n",
    "vocab = [word[0] for word in vocab]\n",
    "vocab.insert(0,\"[UNK]\")\n",
    "vocab[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Using ``hmmlearn.hmm.MultinomialHMM``, create an HMM model ``pos_model`` for POS tagging. It will have POS tags as hidden states and English word tokens as outputs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_model = MultinomialHMM(n_components=len(pos_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  * Set ``pos_model.startprob_`` by using the fraction of sentences in the training data starting with a word with each given POS tag (i.e. what fraction of sentences start with a noun? What fraction of them start with a verb? Etc.) Hint: This should be a list of length ``len(pos_tags)``, of probabilities which sum to one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**source:** https://www.nltk.org/book/ch05.html#ref-dict-to-list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of non-unique tags in train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "929265"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = [tag for item in train for word, tag in item]\n",
    "len(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOUN', '.', 'DET', 'NOUN', 'VERB', 'ADP', 'DET', 'NOUN', 'ADP', 'DET']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionary tag: count sorted by key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.': 4091,\n",
       " 'ADJ': 1558,\n",
       " 'ADP': 5678,\n",
       " 'ADV': 4136,\n",
       " 'CONJ': 2222,\n",
       " 'DET': 9771,\n",
       " 'NOUN': 6465,\n",
       " 'NUM': 769,\n",
       " 'PRON': 7351,\n",
       " 'PRT': 1700,\n",
       " 'VERB': 2113,\n",
       " 'X': 18}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_count={ k:0 for k in sorted(pos_tags) }\n",
    "for tup in train:\n",
    "    tags_count[tup[0][1]] +=1\n",
    "tags_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definding the dictionary with tag probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.': 0.08918294384373911,\n",
       " 'ADJ': 0.03396407394489013,\n",
       " 'ADP': 0.12377921171956749,\n",
       " 'ADV': 0.09016393442622951,\n",
       " 'CONJ': 0.048439134984304154,\n",
       " 'DET': 0.21300575514475062,\n",
       " 'NOUN': 0.14093564701778863,\n",
       " 'NUM': 0.016764039065224973,\n",
       " 'PRON': 0.16025026159748867,\n",
       " 'PRT': 0.037059644227415416,\n",
       " 'VERB': 0.04606295779560516,\n",
       " 'X': 0.00039239623299616326}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = ({k:v/len(train) for k,v in sorted(tags_count.items())})\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing into pos_model.startprob_ values of the dictionary with tag probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.08918294384373911,\n",
       " 0.03396407394489013,\n",
       " 0.12377921171956749,\n",
       " 0.09016393442622951,\n",
       " 0.048439134984304154,\n",
       " 0.21300575514475062,\n",
       " 0.14093564701778863,\n",
       " 0.016764039065224973,\n",
       " 0.16025026159748867,\n",
       " 0.037059644227415416,\n",
       " 0.04606295779560516,\n",
       " 0.00039239623299616326]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_model.startprob_ = list(f.values())\n",
    "pos_model.startprob_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for sum == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(pos_model.startprob_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### * Set ``pos_model.transmat_`` to be the probabilities of transitioning from one POS tag to another, estimated from the training data samples. Hint: This is a matrix of shape ``(len(pos_tags), len(pos_tags))`` whose rows each sum to one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**source:** https://stackoverflow.com/questions/47297585/building-a-transition-matrix-using-words-in-python-numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the dataframe - transition matrix for tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>.</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>ADP</th>\n",
       "      <th>ADV</th>\n",
       "      <th>CONJ</th>\n",
       "      <th>DET</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>NUM</th>\n",
       "      <th>PRON</th>\n",
       "      <th>PRT</th>\n",
       "      <th>VERB</th>\n",
       "      <th>X</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.141134</td>\n",
       "      <td>0.041849</td>\n",
       "      <td>0.112263</td>\n",
       "      <td>0.077801</td>\n",
       "      <td>0.087984</td>\n",
       "      <td>0.148648</td>\n",
       "      <td>0.136508</td>\n",
       "      <td>0.018476</td>\n",
       "      <td>0.107053</td>\n",
       "      <td>0.032556</td>\n",
       "      <td>0.094405</td>\n",
       "      <td>0.001322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADJ</th>\n",
       "      <td>0.099449</td>\n",
       "      <td>0.056864</td>\n",
       "      <td>0.087948</td>\n",
       "      <td>0.009754</td>\n",
       "      <td>0.038283</td>\n",
       "      <td>0.006034</td>\n",
       "      <td>0.653348</td>\n",
       "      <td>0.007065</td>\n",
       "      <td>0.004033</td>\n",
       "      <td>0.019388</td>\n",
       "      <td>0.017371</td>\n",
       "      <td>0.000463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADP</th>\n",
       "      <td>0.009819</td>\n",
       "      <td>0.082389</td>\n",
       "      <td>0.019923</td>\n",
       "      <td>0.015578</td>\n",
       "      <td>0.001836</td>\n",
       "      <td>0.454913</td>\n",
       "      <td>0.259375</td>\n",
       "      <td>0.030242</td>\n",
       "      <td>0.069570</td>\n",
       "      <td>0.014336</td>\n",
       "      <td>0.041518</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADV</th>\n",
       "      <td>0.170299</td>\n",
       "      <td>0.136608</td>\n",
       "      <td>0.142368</td>\n",
       "      <td>0.097114</td>\n",
       "      <td>0.017635</td>\n",
       "      <td>0.073608</td>\n",
       "      <td>0.032846</td>\n",
       "      <td>0.013321</td>\n",
       "      <td>0.047456</td>\n",
       "      <td>0.028531</td>\n",
       "      <td>0.240126</td>\n",
       "      <td>0.000089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONJ</th>\n",
       "      <td>0.020965</td>\n",
       "      <td>0.113317</td>\n",
       "      <td>0.072889</td>\n",
       "      <td>0.091993</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.150578</td>\n",
       "      <td>0.242571</td>\n",
       "      <td>0.019300</td>\n",
       "      <td>0.067207</td>\n",
       "      <td>0.025047</td>\n",
       "      <td>0.195448</td>\n",
       "      <td>0.000457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DET</th>\n",
       "      <td>0.012607</td>\n",
       "      <td>0.239929</td>\n",
       "      <td>0.009202</td>\n",
       "      <td>0.017582</td>\n",
       "      <td>0.000621</td>\n",
       "      <td>0.005961</td>\n",
       "      <td>0.626099</td>\n",
       "      <td>0.009850</td>\n",
       "      <td>0.009877</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.064804</td>\n",
       "      <td>0.001369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOUN</th>\n",
       "      <td>0.284059</td>\n",
       "      <td>0.012848</td>\n",
       "      <td>0.245452</td>\n",
       "      <td>0.026670</td>\n",
       "      <td>0.059999</td>\n",
       "      <td>0.016209</td>\n",
       "      <td>0.150256</td>\n",
       "      <td>0.008055</td>\n",
       "      <td>0.020142</td>\n",
       "      <td>0.018062</td>\n",
       "      <td>0.157885</td>\n",
       "      <td>0.000362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUM</th>\n",
       "      <td>0.270584</td>\n",
       "      <td>0.058681</td>\n",
       "      <td>0.132157</td>\n",
       "      <td>0.020229</td>\n",
       "      <td>0.039789</td>\n",
       "      <td>0.014879</td>\n",
       "      <td>0.381760</td>\n",
       "      <td>0.021650</td>\n",
       "      <td>0.008944</td>\n",
       "      <td>0.005684</td>\n",
       "      <td>0.045390</td>\n",
       "      <td>0.000251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRON</th>\n",
       "      <td>0.103173</td>\n",
       "      <td>0.009442</td>\n",
       "      <td>0.055102</td>\n",
       "      <td>0.053731</td>\n",
       "      <td>0.011371</td>\n",
       "      <td>0.017284</td>\n",
       "      <td>0.008807</td>\n",
       "      <td>0.001142</td>\n",
       "      <td>0.008629</td>\n",
       "      <td>0.024162</td>\n",
       "      <td>0.707132</td>\n",
       "      <td>0.000025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRT</th>\n",
       "      <td>0.076952</td>\n",
       "      <td>0.018685</td>\n",
       "      <td>0.091216</td>\n",
       "      <td>0.035494</td>\n",
       "      <td>0.011929</td>\n",
       "      <td>0.083375</td>\n",
       "      <td>0.036745</td>\n",
       "      <td>0.004963</td>\n",
       "      <td>0.007257</td>\n",
       "      <td>0.011845</td>\n",
       "      <td>0.621538</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VERB</th>\n",
       "      <td>0.080627</td>\n",
       "      <td>0.057457</td>\n",
       "      <td>0.169356</td>\n",
       "      <td>0.103482</td>\n",
       "      <td>0.014479</td>\n",
       "      <td>0.163062</td>\n",
       "      <td>0.097818</td>\n",
       "      <td>0.009109</td>\n",
       "      <td>0.055306</td>\n",
       "      <td>0.065710</td>\n",
       "      <td>0.183417</td>\n",
       "      <td>0.000178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>0.270577</td>\n",
       "      <td>0.001892</td>\n",
       "      <td>0.055818</td>\n",
       "      <td>0.007569</td>\n",
       "      <td>0.022706</td>\n",
       "      <td>0.006623</td>\n",
       "      <td>0.054872</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>0.006623</td>\n",
       "      <td>0.006623</td>\n",
       "      <td>0.060549</td>\n",
       "      <td>0.505203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0         .       ADJ       ADP       ADV      CONJ       DET      NOUN  \\\n",
       "row_0                                                                         \n",
       ".      0.141134  0.041849  0.112263  0.077801  0.087984  0.148648  0.136508   \n",
       "ADJ    0.099449  0.056864  0.087948  0.009754  0.038283  0.006034  0.653348   \n",
       "ADP    0.009819  0.082389  0.019923  0.015578  0.001836  0.454913  0.259375   \n",
       "ADV    0.170299  0.136608  0.142368  0.097114  0.017635  0.073608  0.032846   \n",
       "CONJ   0.020965  0.113317  0.072889  0.091993  0.000229  0.150578  0.242571   \n",
       "DET    0.012607  0.239929  0.009202  0.017582  0.000621  0.005961  0.626099   \n",
       "NOUN   0.284059  0.012848  0.245452  0.026670  0.059999  0.016209  0.150256   \n",
       "NUM    0.270584  0.058681  0.132157  0.020229  0.039789  0.014879  0.381760   \n",
       "PRON   0.103173  0.009442  0.055102  0.053731  0.011371  0.017284  0.008807   \n",
       "PRT    0.076952  0.018685  0.091216  0.035494  0.011929  0.083375  0.036745   \n",
       "VERB   0.080627  0.057457  0.169356  0.103482  0.014479  0.163062  0.097818   \n",
       "X      0.270577  0.001892  0.055818  0.007569  0.022706  0.006623  0.054872   \n",
       "\n",
       "col_0       NUM      PRON       PRT      VERB         X  \n",
       "row_0                                                    \n",
       ".      0.018476  0.107053  0.032556  0.094405  0.001322  \n",
       "ADJ    0.007065  0.004033  0.019388  0.017371  0.000463  \n",
       "ADP    0.030242  0.069570  0.014336  0.041518  0.000500  \n",
       "ADV    0.013321  0.047456  0.028531  0.240126  0.000089  \n",
       "CONJ   0.019300  0.067207  0.025047  0.195448  0.000457  \n",
       "DET    0.009850  0.009877  0.002100  0.064804  0.001369  \n",
       "NOUN   0.008055  0.020142  0.018062  0.157885  0.000362  \n",
       "NUM    0.021650  0.008944  0.005684  0.045390  0.000251  \n",
       "PRON   0.001142  0.008629  0.024162  0.707132  0.000025  \n",
       "PRT    0.004963  0.007257  0.011845  0.621538  0.000000  \n",
       "VERB   0.009109  0.055306  0.065710  0.183417  0.000178  \n",
       "X      0.000946  0.006623  0.006623  0.060549  0.505203  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct = pd.crosstab(pd.Series(tags[:-1]),pd.Series(tags[1:]),normalize=0)\n",
    "ct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing down the values into pos_model.transmat_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_model.transmat_ = ct.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for `sum of row` = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_model.transmat_[0].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Check the shape is the same as in a question's hint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(pos_tags), len(pos_tags)) == (pos_model.transmat_).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  * Set ``pos_model.emissionprob_`` to be the probabilities of outputting each word in ``vocab`` given a POS tag. Estimate this from the training data samples, making sure to make all words lowercase and to replace words that are not in ``vocab`` with \"\\[UNK\\]\". Hint: This is a matrix of shape ``(len(pos_tags), len(vocab))`` whose rows each sum to one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the zero matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "emission_matrix = defaultdict(int,{k:np.zeros(len(pos_tags)) for k in vocab})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the dictionary with indx for tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_indx = defaultdict(int,{k:i for i,k in enumerate(sorted(pos_tags))})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the dictionary with indx for words in vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_indx = defaultdict(int,{k:i for i,k in enumerate(vocab)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set pos_model.emissionprob_ to be the probabilities of outputting each word in vocab given a POS tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tup in train:\n",
    "    for wrd, tg in tup:\n",
    "        tag = tag_indx[tg]\n",
    "        if wrd.lower() in emission_matrix.keys():\n",
    "            emission_matrix[wrd.lower()][tag] +=1\n",
    "        else:\n",
    "            emission_matrix['[UNK]'][tag] +=1\n",
    "\n",
    "for key in emission_matrix.keys():\n",
    "    summ = np.sum(emission_matrix[key])\n",
    "    if summ == 0:\n",
    "        print(key, emission_matrix[key])\n",
    "    else:\n",
    "        emission_matrix[key] = emission_matrix[key]/summ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the sum == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(list(emission_matrix.values()))[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_model.emissionprob_ = np.array(list(emission_matrix.values())).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the shape of emissionprob matrix with conditions in question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 5001)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_model.emissionprob_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_model.emissionprob_.shape == (len(pos_tags), len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Make a function ``get_pos(sentence)`` that returns the most likely POS tags for the words in the string ``sentence``, calculated using pos_model.decode(...). Apply this to a few (lowercase, no punctuation) sentences including: \"this is a test\", \"tel aviv is in israel\", \"i know how to write code\". \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos(sentence):\n",
    "    sentence_list = sentence.split() \n",
    "    x = np.array([words_indx[word] for word in sentence_list]).reshape(-1, 1)\n",
    "    _, tag_decode = pos_model.decode(x)\n",
    "    pos_list =[]\n",
    "    for i in tag_decode:\n",
    "        pos_list.append([u for u in tag_indx if tag_indx[u] == i])\n",
    "    return pos_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['DET'], ['VERB'], ['DET'], ['NOUN']]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_pos(\"this is a test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['NOUN'], ['NOUN'], ['VERB'], ['ADP'], ['NOUN']]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_pos(\"tel aviv is in israel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['PRON'], ['VERB'], ['ADV'], ['PRT'], ['VERB'], ['NOUN']]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_pos(\"i know how to write code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do the results look reasonable to you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well,  yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## BONUS:\n",
    "### Calculate the accuracy (proportion of POS tags predicted correctly) on the testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ctreating word_list and tags_list from test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_test = []\n",
    "tags_test = []\n",
    "for x in test:\n",
    "    for w in x:\n",
    "        if w[0].lower() in vocab:\n",
    "            words_test.append(w[0].lower())\n",
    "        else:\n",
    "            words_test.append('[UNK]')\n",
    "        tags_test.append(w[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list = []\n",
    "\n",
    "x = np.array([words_indx[k] for k in words_test]).reshape(-1, 1)\n",
    "_, tag_decode = pos_model.decode(x)\n",
    "\n",
    "for i in tag_decode:\n",
    "    pos_list.append([u for u in tag_indx if tag_indx[u] == i])\n",
    "\n",
    "\n",
    "y_pred = pos_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy On The Testing Data. 0.92\n"
     ]
    }
   ],
   "source": [
    "print('The Accuracy On The Testing Data.',round(accuracy_score(tags_test, y_pred),2))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO186EKp0f9p831+ab+ROrQ",
   "collapsed_sections": [],
   "name": "HMM exercise.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
